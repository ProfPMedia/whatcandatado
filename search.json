[
  {
    "objectID": "posts/Data vs. Information/index.html",
    "href": "posts/Data vs. Information/index.html",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "",
    "text": "Data Alchemy\nTL;DR: “What Can Data Do? The Alchemy of Transforming Data into Information”\nIn the landscape of ‘big data,’ the pressing question “What Can Data Do?” is more than a rhetorical one—it’s a call to action for anyone immersed in the digital age. Data is omnipresent, flowing through every click, transaction, and interaction. But its abundance alone isn’t what gives it value; it’s the transformation of this raw data into information that empowers businesses, informs decisions, and ignites innovation.\nThis transformation is a meticulous process, guided by the rigorous application of statistics. It’s how we decode the vastness of data into insights that can make a business more efficient, reveal the hidden patterns in user behavior, or even predict future market trends. Yet, this process is not without its pitfalls. Missteps in statistical interpretation can lead to false conclusions, misleading representations, and poor decisions.\nWhy, then, should we care about data? Because when interpreted correctly, it becomes the most trustworthy source of knowledge in a world that’s increasingly driven by information. Data is not just a sequence of numbers; it’s a narrative waiting to be read, offering a glimpse into the unseen, and a foundation for strategic foresight.\nIn this post, we’ll explore the transformative power of data when wielded with statistical acumen, and why understanding this transformation is crucial for anyone looking to thrive in a data-driven reality."
  },
  {
    "objectID": "posts/Data vs. Information/index.html#the-power-of-data-transformation",
    "href": "posts/Data vs. Information/index.html#the-power-of-data-transformation",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "The Power of Data Transformation",
    "text": "The Power of Data Transformation\nData is everywhere. It’s the raw input, the unprocessed facts and figures that surround us in daily life. But in its raw state, data is like an uncut gem—it has potential but lacks refinement.\nInformation, on the other hand, is data that has been processed, organized, and contextualized to gain meaning. It tells a story, answers a question, or solves a problem. It’s data made usable, transformed through analysis to inform and guide.\nStatistics is the craftsman that turns the raw data into gleaming information. It’s a toolset comprising methods and techniques that allow us to extract meaning from data. Through descriptive statistics, we can summarize data, providing a snapshot of its shape and nature. Inferential statistics take us a step further, enabling us to make predictions or inferences about a larger population from a sample.\nReal-world examples of data-driven decision-making abound. In healthcare, data analysis predicts disease outbreaks and improves patient outcomes. Retailers use data to predict inventory needs and customer preferences. Sports analytics inform coaching decisions and game strategies. In each case, data alone offers potential; it’s the transformation into information that unlocks value.\nIn the business world, efficiency is paramount. Data transformation directly impacts efficiency by enabling better decision-making. With accurate information, businesses can streamline operations, reduce waste, and target resources effectively."
  },
  {
    "objectID": "posts/Data vs. Information/index.html#uncovering-hidden-patterns",
    "href": "posts/Data vs. Information/index.html#uncovering-hidden-patterns",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "Uncovering Hidden Patterns",
    "text": "Uncovering Hidden Patterns\nThe ability to detect trends and patterns within vast datasets is one of the most compelling aspects of data analysis. By applying statistical models and algorithms to data, analysts can uncover trends that are not immediately apparent.\nMachine learning, a subset of artificial intelligence, takes pattern detection to a new level. It uses data to train algorithms, allowing them to make predictions or take actions without being explicitly programmed to perform the task.\nReal-world case studies highlight the power of pattern recognition through data analysis. Credit card companies use algorithms to detect fraudulent transactions. In agriculture, data analysis helps farmers understand soil conditions and crop cycles, leading to better yield predictions."
  },
  {
    "objectID": "posts/Data vs. Information/index.html#the-pitfalls-of-data-interpretation",
    "href": "posts/Data vs. Information/index.html#the-pitfalls-of-data-interpretation",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "The Pitfalls of Data Interpretation",
    "text": "The Pitfalls of Data Interpretation\nThe path from data to information is fraught with potential missteps. Misinterpreting correlation as causation, misuse of statistical significance, sampling bias, and confirmation bias are common errors that can lead to misleading conclusions.\nThe consequences of these statistical misinterpretations can be significant. In business, they can lead to flawed strategies and wasted resources. In policy-making, they can result in ineffective or even harmful policies.\nAvoiding these pitfalls requires a robust statistical methodology and a critical approach to data analysis. Transparency, peer review, and replication are vital, as they allow others to verify results and ensure that conclusions are not the result of chance or error."
  },
  {
    "objectID": "posts/Data vs. Information/index.html#the-why-behind-data",
    "href": "posts/Data vs. Information/index.html#the-why-behind-data",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "The ‘Why’ Behind Data",
    "text": "The ‘Why’ Behind Data\nData’s intrinsic value lies in its ability to be transformed into insights that can drive innovation, inform strategy, and improve outcomes. It’s a tool for strategic decision-making and has a critical role in education and learning.\nEthical considerations are paramount when dealing with data, especially personal or sensitive information. Ensuring fairness and equity in data analysis is not just a technical challenge but a moral imperative."
  },
  {
    "objectID": "posts/Data vs. Information/index.html#data-as-a-bridge-to-innovation",
    "href": "posts/Data vs. Information/index.html#data-as-a-bridge-to-innovation",
    "title": "The Alchemy of Transforming Data into Information",
    "section": "Data as a Bridge to Innovation",
    "text": "Data as a Bridge to Innovation\nData is the linchpin of innovation, driving progress in healthcare, automotive, energy, and finance industries. It shapes future technologies like IoT, AI, and machine learning and spurs continuous improvement and growth in business processes and operations."
  },
  {
    "objectID": "posts/DoingDataWranglingRight/ddar.html",
    "href": "posts/DoingDataWranglingRight/ddar.html",
    "title": "Doing Data Analysis Right!",
    "section": "",
    "text": "Course materials for Doing Data Analysis Right\nInspiration for the Course - Good Data Analysis - Reproducible Data Analysis in Jupyter - Our World In Data\nPrerequisites - Familiarity with a programming language, preferably Python, although the code will be accessible, so if you have exposure (proficiency) with another language, you should be able to follow along just fine. - Basic understanding of descriptive statistics like a mean, median, or standard deviation. - An keen interest in the methods and art form of doing data analysis\nReferences - Python - Pandas - Plotly Express\nAfter finishing this course you will be able to analyze a complex dataset of your choosing for the purpose of informing better decision making. This will be facilitated by an illustrated end to end example with clear and consise description of how and why to perform each step of the analysis process and ultimately communicate your results to a non-technical audience."
  },
  {
    "objectID": "posts/DoingDataWranglingRight/ddar.html#link-to-the-data-analysis-report",
    "href": "posts/DoingDataWranglingRight/ddar.html#link-to-the-data-analysis-report",
    "title": "Doing Data Analysis Right!",
    "section": "Link to the Data Analysis Report",
    "text": "Link to the Data Analysis Report\nAutomated Data Analysis Report\n\nmy_report = sv.analyze(df)\nmy_report.show_html()\n\n\n\n\nReport SWEETVIZ_REPORT.html was generated! NOTEBOOK/COLAB USERS: the web browser MAY not pop up, regardless, the report IS saved in your notebook/colab files."
  },
  {
    "objectID": "posts/DoingDataWranglingRight/ddar.html#data-distributions",
    "href": "posts/DoingDataWranglingRight/ddar.html#data-distributions",
    "title": "Doing Data Analysis Right!",
    "section": "Data Distributions",
    "text": "Data Distributions\nA key step in Data Mise en Place is the exploration of data distributions. Many time a careless analyst will rely just on the use of descriptive statistics like mean, variance, median, etc. but this leave them vulnerable to missing the underlying character of the data. The key insight (experience) is there are an infinite number of data sets that have the same mean although, they may (and usually do) have radically different distributions and most importantly outliers.\nDefinition: Data distributions refer to the way data points are spread or distributed across different values in a dataset. Understanding data distributions is fundamental in data analysis as it allows us to gain insights into the central tendencies, variations, and outliers present in the data.\nImportance: Analyzing data distributions is essential because it provides a clear and concise summary of the dataset’s characteristics. It helps us identify the most common values, assess the spread of data, and detect any unusual patterns or extreme values. Having a thorough understanding of data distributions aids in making informed decisions about data transformations, choosing appropriate statistical tests, and selecting the right models for predictive analysis.\nHow to Perform on Data:\n\nHistograms: A histogram is a graphical representation of the data distribution, showing the frequency or count of data points falling into predefined bins or intervals. To create a histogram:\n\nChoose the number of bins or intervals (often denoted as ‘k’) based on the data’s range and size.\nDivide the data range into ‘k’ equal-width intervals.\nCount the number of data points that fall within each interval and plot them as bars.\n\nHistograms provide an immediate visual understanding of data spread and can reveal underlying patterns, such as whether the data is normally distributed, skewed, or multi-modal.\nCumulative Distribution Functions (CDF): A cumulative distribution function is another graphical representation of data distribution that shows the probability that a random data point will be found at or below a given value. To construct a CDF:\n\nSort the data points in ascending order.\nCalculate the cumulative probability for each data point as the fraction of data points below or equal to it.\nPlot the data points against their cumulative probabilities.\n\nCDFs are helpful in understanding percentiles and quantiles, enabling us to answer questions like “What percentage of data falls below a certain value?”\nQ-Q Plots (Quantile-Quantile Plots): Q-Q plots compare the quantiles of a dataset against the quantiles of a theoretical distribution (usually the normal distribution). To create a Q-Q plot:\n\nSort the data in ascending order.\nCalculate the theoretical quantiles from the chosen distribution.\nPlot the data quantiles against the corresponding theoretical quantiles.\n\nQ-Q plots help us assess whether the data follows a specific distribution or if there are significant deviations from it.\n\nBy combining histograms, CDFs, and Q-Q plots in data analysis, we gain a comprehensive understanding of the data distribution, enabling us to make more accurate inferences and build robust models in various fields, including finance, healthcare, and environmental sciences.\n\ngenerate_dist_examples()\n\n\n                                                \n\n\n\n                                                \n\n\n\n                                                \n\n\nNow that we have a sense of how the distributions of data can give us insight. Let’s turn our attention to our working example. Let’s look at the distributions of World GDP in dollars and primary energy consumption in Terawatt hours.\n\ncountries_of_interest = ['Australia', 'Brazil', 'Canada','China', 'France', 'Germany', 'India', 'Japan','Mexico', 'Russia',\n       'Saudi Arabia', 'United Kingdom','United States','Vietnam']\npdf = df[df['country'].isin(countries_of_interest)].dropna()\npdf['dollar_per_capita'] = (pdf['gdp']/pdf['population'])\npdf['kwhr_per_capita'] = (pdf['primary_energy_consumption']*1000000000/pdf['population'])\npdf['dollar_per_kwhr'] = pdf['dollar_per_capita']/pdf['kwhr_per_capita']\npdf.head()\n\n\n\n\n\n\n\n\ncountry\nyear\niso_code\npopulation\ngdp\nprimary_energy_consumption\nfossil_share_energy\nlow_carbon_share_energy\ndollar_per_capita\nkwhr_per_capita\ndollar_per_kwhr\n\n\n\n\n1577\nAustralia\n1965\nAUS\n11359442.0\n1.851121e+11\n425.760\n94.437\n5.563\n16295.881193\n37480.714282\n0.434780\n\n\n1578\nAustralia\n1966\nAUS\n11592675.0\n1.902576e+11\n446.102\n94.928\n5.072\n16411.878879\n38481.368623\n0.426489\n\n\n1579\nAustralia\n1967\nAUS\n11809114.0\n2.031107e+11\n473.093\n95.050\n4.950\n17199.486338\n40061.684560\n0.429325\n\n\n1580\nAustralia\n1968\nAUS\n12027268.0\n2.150465e+11\n501.694\n95.135\n4.865\n17879.912915\n41713.047385\n0.428641\n\n\n1581\nAustralia\n1969\nAUS\n12268351.0\n2.281273e+11\n522.428\n94.913\n5.087\n18594.780459\n42583.392014\n0.436667\n\n\n\n\n\n\n\n\n# Creating the scatter plot using Plotly Express\nfig = px.scatter(pdf, x='gdp', y='primary_energy_consumption', color='country', hover_data=['iso_code', 'year','gdp', 'primary_energy_consumption'],\n                 labels={'X': 'GDP in $', 'Y': 'Energy Consumption TWHrs'},\n                 title='World GDP vs. Energy Consumption', height=480)\n\n# Display the plot\nfig.show()\n\n\n                                                \n\n\n\n# Create the stacked bar chart using Plotly Express\nfig = px.bar(pdf, x='year', y='dollar_per_kwhr', color='country', barmode='stack',\n             labels={'year': 'year', 'dollar_per_kwhr': 'dollar_per_kwhr', 'country': 'country'},\n             title='GDP output per KWhr per capital', height=640)\n\n# Display the plot\nfig.show()\n\n\n                                                \n\n\n\nimport matplotlib.pyplot as plt\npdf[pdf.country=='Vietnam'].plot.bar(x='year', y='dollar_per_kwhr')\nplt.xlabel('year')\nplt.ylabel('GDP output per capita per kwhr')\nplt.show()"
  },
  {
    "objectID": "posts/DoingDataAtScale/Building a Data Intensive Applications/index.html",
    "href": "posts/DoingDataAtScale/Building a Data Intensive Applications/index.html",
    "title": "Building Data Intensive Applications - forecasting bike inventory",
    "section": "",
    "text": "Citibike\n\n\n\n# Coming in the spring of 2024"
  },
  {
    "objectID": "posts/DoingDataAtScale/MLflow/index.html",
    "href": "posts/DoingDataAtScale/MLflow/index.html",
    "title": "MLflow - Overview",
    "section": "",
    "text": "MLflow"
  },
  {
    "objectID": "posts/DoingDataAtScale/MLflow/index.html#key-features-of-mlflow",
    "href": "posts/DoingDataAtScale/MLflow/index.html#key-features-of-mlflow",
    "title": "MLflow - Overview",
    "section": "Key Features of MLflow:",
    "text": "Key Features of MLflow:\n\nOpen Interface Design: MLflow is designed with an open interface, allowing data scientists and engineers to bring their own training code, metrics, and inference logic while benefiting from a structured development process.\nThree Main Components:\n\nMLflow Tracking: An API for recording experiment runs, including code used, parameters, input data, metrics, and arbitrary output files. This helps in keeping track of various experiments and their results.\nMLflow Projects: A format for packaging code into reusable projects. Each project defines its environment, the code to run, and parameters. It ensures reproducibility by packaging the entire environment and dependencies.\nMLflow Models: A generic format for packaging machine learning models in multiple formats, allowing diverse tools to understand the model at different levels of abstraction. It supports various deployment tools like batch, real-time inference, and cloud platforms.\n\nExperiment Tracking: MLflow provides a UI for tracking and visualizing experiments, allowing users to organize, search, sort, and compare runs.\nReproducibility: MLflow Projects ensure that the same environment and dependencies are used, making it easy to reproduce results.\nModel Packaging: MLflow Models allow for the packaging of both the model and any custom logic, ensuring that they can be deployed and tested as a single unit.\n\n\nProblems MLflow Solves:\n\nMultitude of Tools: MLflow addresses the challenge of using multiple tools at each phase of ML development.\nExperiment Tracking: It provides a solution for tracking which parameters, code, and data went into each experiment.\nReproducibility: MLflow ensures that results can be reproduced, addressing challenges in getting the same code to work consistently.\nProduction Deployment: It simplifies the process of moving an application to production, ensuring that models are deployed reliably.\n\n\n\nSignificance of MLflow:\n\nFlexibility: Unlike other platforms that restrict users to specific algorithms or libraries, MLflow’s open interface design provides flexibility while retaining the benefits of lifecycle management.\nStructured ML Lifecycle: MLflow structures the machine learning lifecycle, ensuring reproducibility, deployability, and a standardized development process."
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html",
    "title": "Delta Lake - Overview",
    "section": "",
    "text": "Delta Lake"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#key-features-of-delta-lake",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#key-features-of-delta-lake",
    "title": "Delta Lake - Overview",
    "section": "Key Features of Delta Lake:",
    "text": "Key Features of Delta Lake:\n\nACID Transactions: Delta Lake provides full ACID transactional capabilities, ensuring data integrity and consistency.\nScalable Metadata Handling: It uses a unique approach to handle metadata, allowing it to scale to billions of partitions and trillions of files.\nUnified Batch and Streaming: Delta Lake supports both batch and streaming workloads with a unified experience.\nSchema Evolution: It allows for schema-on-read and schema-on-write, enabling users to evolve their datasets over time.\nAudit History: Delta Lake maintains a detailed audit trail of all changes, allowing for versioning and time-travel capabilities.\nSupport for Deletes, Updates, and Merges: Delta Lake supports operations like DELETE, UPDATE, and MERGE INTO, which are typically not available in most data lakes."
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#problems-delta-lake-solves",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#problems-delta-lake-solves",
    "title": "Delta Lake - Overview",
    "section": "Problems Delta Lake Solves:",
    "text": "Problems Delta Lake Solves:\n\nData Reliability: Traditional data lakes often suffer from issues like data corruption, missing files, or duplicate data. Delta Lake’s ACID transactions eliminate these problems.\nPerformance: By leveraging Spark’s capabilities and optimizing storage layer operations, Delta Lake provides faster query performance.\nComplex ETL Workflows: Delta Lake simplifies ETL workflows by supporting operations like upserts and deletes.\nMetadata Scalability: Handling metadata for large datasets can be challenging. Delta Lake’s approach to metadata ensures scalability and performance."
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#significance-of-delta-lake-in-the-data-stack",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#significance-of-delta-lake-in-the-data-stack",
    "title": "Delta Lake - Overview",
    "section": "Significance of Delta Lake in the Data Stack:",
    "text": "Significance of Delta Lake in the Data Stack:\n\nEnhanced Data Reliability: By providing ACID transactions, Delta Lake ensures that the data is reliable and consistent, which is crucial for analytical workloads.\nFlexibility: With support for schema evolution and a wide range of operations, Delta Lake offers flexibility in managing and processing data.\nIntegration with Existing Tools: Delta Lake seamlessly integrates with existing tools and frameworks, ensuring that organizations can adopt it without significant changes to their existing infrastructure."
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#integration-with-spark",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#integration-with-spark",
    "title": "Delta Lake - Overview",
    "section": "Integration with Spark:",
    "text": "Integration with Spark:\n\nNative Integration: Delta Lake is built on top of Spark, ensuring native integration and optimization.\nOptimized Query Execution: Delta Lake leverages Spark’s Catalyst optimizer for efficient query execution.\nUse of Spark APIs: Users can utilize familiar Spark APIs to read and write data in Delta Lake, ensuring a smooth user experience.\n\nDelta Lake is a significant addition to the data stack as it addresses many of the challenges faced by traditional data lakes. By providing ACID transactions, scalable metadata handling, and a range of operations, Delta Lake ensures that data is reliable, consistent, and easily accessible. Its deep integration with Spark further enhances its capabilities, making it a powerful tool for data processing and analytics.\nACID is an acronym that stands for Atomicity, Consistency, Isolation, and Durability. These are a set of properties that ensure reliable processing of database transactions. Let’s delve into each of these properties:\n\n1. Atomicity:\nDefinition: Atomicity ensures that a transaction is treated as a single, indivisible unit, which means either all of its operations are executed or none of them are. If a transaction is interrupted (for example, due to a system crash or power failure), any changes that it made are rolled back to ensure the database remains in a consistent state.\nExample: Consider a banking application where you are transferring money from one account to another. This transaction involves two operations: - Deducting the amount from the source account. - Adding the amount to the destination account.\nIf the system fails after deducting the amount but before adding it to the destination account, atomicity ensures that the deducted amount is rolled back to the source account, ensuring no money is lost.\n\n\n2. Consistency:\nDefinition: Consistency ensures that a transaction brings the database from one valid state to another. After a transaction has been committed, the changes are permanent, and the database will be left in a consistent state. Any transaction that would violate the database’s consistency rules is rolled back.\nExample: In the same banking application, suppose there’s a rule that an account balance should never go below $100. If a transaction tries to withdraw an amount that would violate this rule, the transaction would be rolled back, ensuring the database remains consistent.\n\n\n3. Isolation:\nDefinition: Isolation ensures that concurrent transactions are executed in such a way that the results are the same as if the transactions were executed serially, one after the other. This means that the intermediate state of a transaction is invisible to other transactions.\nExample: Imagine two transactions: - Transaction A reads a value and increases it by $10. - Transaction B reads the same value simultaneously and increases it by $20.\nIf these transactions are not isolated, they might both read the same initial value, say $100, and update it to $110 and $120, respectively. With proper isolation, the final value would be $130, as one transaction would wait for the other to complete before reading the updated value.\n\n\n4. Durability:\nDefinition: Durability ensures that once a transaction has been committed, its effects are permanent, even in the case of system failures. This is typically achieved by storing transaction logs or using backup mechanisms.\nExample: After completing a purchase in an online store, the transaction details are written to the database. Even if the system crashes immediately after, durability ensures that the transaction details are not lost and can be recovered when the system restarts.\nACID properties are fundamental to ensuring the reliability and robustness of database systems. They ensure that the data remains consistent and intact even in the face of system failures or concurrent access.\n\nimport pyspark\nfrom delta import *\nbuilder = (\n    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n)\nspark = configure_spark_with_delta_pip(builder).getOrCreate()"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#create-a-delta-table",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#create-a-delta-table",
    "title": "Delta Lake - Overview",
    "section": "Create a Delta Table",
    "text": "Create a Delta Table\n\nUnderlying format uses columnar Parquet Files\nJSON delta log is used by the delta lake transactional core software\n\n\ndata = spark.range(0, 5)\ndata.write.format(\"delta\").save(\"./delta-table\", OVERWRITE=True)\n\n                                                                                \n\n\n\n!ls delta-table/\n\n_delta_log\npart-00000-aa7ed3d3-662a-46cf-b94f-f90235a78d46-c000.snappy.parquet\npart-00001-c57f57a0-c72a-43ad-8529-a37859acd44f-c000.snappy.parquet\npart-00002-50f9305f-41cc-43e6-b532-7f4c50233b3f-c000.snappy.parquet\npart-00003-19ed009f-69d1-49ce-86f1-312cb074b6a3-c000.snappy.parquet"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#read-the-delta-table",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#read-the-delta-table",
    "title": "Delta Lake - Overview",
    "section": "Read the delta Table",
    "text": "Read the delta Table\n\ndf = spark.read.format(\"delta\").load(\"./delta-table\")\ndf.show()\n\n+---+\n| id|\n+---+\n|  3|\n|  4|\n|  2|\n|  0|\n|  1|\n+---+"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#update-table---overwrite",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#update-table---overwrite",
    "title": "Delta Lake - Overview",
    "section": "Update table - overwrite",
    "text": "Update table - overwrite\n\ndata = spark.range(5, 10)\ndata.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-table\")\n\n                                                                                \n\n\n\ndf = spark.read.format(\"delta\").load(\"./delta-table\")\ndf.show()\n\n+---+\n| id|\n+---+\n|  8|\n|  9|\n|  5|\n|  6|\n|  7|\n+---+"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#conditional-update-without-overwrite",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#conditional-update-without-overwrite",
    "title": "Delta Lake - Overview",
    "section": "Conditional update without overwrite",
    "text": "Conditional update without overwrite\n\nfrom delta.tables import *\nfrom pyspark.sql.functions import *\n\n\ndeltaTable = DeltaTable.forPath(spark, \"./delta-table\")\n\n\n# Update every even value by adding 100 to it\ndeltaTable.update(condition=expr(\"id % 2 == 0\"), set={\"id\": expr(\"id + 100\")})\n\n                                                                                \n\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n|108|\n|  9|\n|  5|\n|  7|\n|106|\n+---+\n\n\n\n\n# Delete every even value\ndeltaTable.delete(condition=expr(\"id % 2 == 0\"))\n\n                                                                                \n\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n|  9|\n|  5|\n|  7|\n+---+\n\n\n\n\n# Upsert (merge) new data\nnewData = spark.range(0, 20)\n\ndeltaTable.alias(\"oldData\").merge(\n    newData.alias(\"newData\"), \"oldData.id = newData.id\"\n).whenMatchedUpdate(set={\"id\": col(\"newData.id\")}).whenNotMatchedInsert(\n    values={\"id\": col(\"newData.id\")}\n).execute()\n\n                                                                                \n\n\n\ndeltaTable.toDF().show()\n\n+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n| 10|\n| 11|\n| 12|\n| 13|\n| 14|\n| 15|\n| 16|\n| 17|\n| 18|\n| 19|\n+---+"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#read-older-versions-of-data-using-time-travel",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#read-older-versions-of-data-using-time-travel",
    "title": "Delta Lake - Overview",
    "section": "Read older versions of data using time travel",
    "text": "Read older versions of data using time travel\n\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./delta-table\")\ndf.show()\n\n                                                                                \n\n\n+---+\n| id|\n+---+\n|  3|\n|  4|\n|  2|\n|  0|\n|  1|\n+---+"
  },
  {
    "objectID": "posts/DoingDataAtScale/Delta Lake/index.html#looking-at-the-underlying-transaction-log-maintained-by-delta-lake",
    "href": "posts/DoingDataAtScale/Delta Lake/index.html#looking-at-the-underlying-transaction-log-maintained-by-delta-lake",
    "title": "Delta Lake - Overview",
    "section": "Looking at the underlying transaction log maintained by Delta Lake",
    "text": "Looking at the underlying transaction log maintained by Delta Lake\n\n!cat ./delta-table/_delta_log/00000000000000000003.json\n\n{\"remove\":{\"path\":\"part-00000-ad39a0c3-57cd-4b19-bd34-c1322d1f026e-c000.snappy.parquet\",\"deletionTimestamp\":1698778757438,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":486}}\n{\"remove\":{\"path\":\"part-00001-a61ba60f-f4eb-40c1-ab50-88fa3600e94a-c000.snappy.parquet\",\"deletionTimestamp\":1698778757438,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":478}}\n{\"add\":{\"path\":\"part-00000-dbf8c991-8a1a-42b4-9b3e-93dcb13801fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":478,\"modificationTime\":1698778757430,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":9},\\\"maxValues\\\":{\\\"id\\\":9},\\\"nullCount\\\":{\\\"id\\\":0}}\"}}\n{\"commitInfo\":{\"timestamp\":1698778757453,\"operation\":\"DELETE\",\"operationParameters\":{\"predicate\":\"[\\\"((id % 2L) = 0L)\\\"]\"},\"readVersion\":2,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numRemovedFiles\":\"2\",\"numCopiedRows\":\"1\",\"numAddedChangeFiles\":\"0\",\"executionTimeMs\":\"899\",\"numDeletedRows\":\"2\",\"scanTimeMs\":\"722\",\"numAddedFiles\":\"1\",\"rewriteTimeMs\":\"177\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.0\",\"txnId\":\"951deaaf-e06b-464a-bbf1-a2e6bc62efb7\"}}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Series",
    "section": "",
    "text": "Data Series\n\nDoing Data at Scale\nData Science @ Scale with Spark, Delta Lake, and MLflow.\n\n\n\nData Posts\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nWhat Can Data Do?\n\n\n2 min\n\n\n\nnews\n\n\n\n\n\n\n\nJul 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDoing Data Analysis Right!\n\n\n11 min\n\n\n\ndata-analysis\n\n\nEDA\n\n\nCourses\n\n\n\n\n\n\n\nSep 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n1 min\n\n\n\nCourses\n\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nData Intensive Applications - Introduction\n\n\n49 min\n\n\n\nDIA\n\n\nCourses\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSpark- Overview\n\n\n6 min\n\n\n\nSpark\n\n\nCourses\n\n\nDSAS\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDelta Lake - Overview\n\n\n5 min\n\n\n\nDeltaLake\n\n\nCourses\n\n\nDSAS\n\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMLflow - Overview\n\n\n2 min\n\n\n\nMLflow\n\n\nCourses\n\n\nDSAS\n\n\nML-OPS\n\n\n\n\n\n\n\nOct 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Intensive Applications - forecasting bike inventory\n\n\n1 min\n\n\n\nDIA\n\n\nCourses\n\n\nDSAS\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Alchemy of Transforming Data into Information\n\n\n5 min\n\n\n\ntheory\n\n\n\n\n\n\n\nNov 4, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "The Alchemy of Transforming Data into Information\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\n\n\n\n\n\n\nBuilding Data Intensive Applications - forecasting bike inventory\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nMLflow - Overview\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\n\n\n\n\n\n\nDelta Lake - Overview\n\n\n\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\nSpark- Overview\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\nData Intensive Applications - Introduction\n\n\n\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\nDoing Data Analysis Right!\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\n\n\n\n\n\n\nWhat Can Data Do?\n\n\n\n\n\n\n\n\n\nJul 24, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "doingdata-series.html",
    "href": "doingdata-series.html",
    "title": "Series: Doing Data at Scale",
    "section": "",
    "text": "Introduction\n\n\n1 min\n\n\n\n\n\n\nSep 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nData Intensive Applications - Introduction\n\n\n49 min\n\n\n\n\n\n\nOct 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSpark- Overview\n\n\n6 min\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDelta Lake - Overview\n\n\n5 min\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMLflow - Overview\n\n\n2 min\n\n\n\n\n\n\nOct 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Data Intensive Applications - forecasting bike inventory\n\n\n1 min\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to What Can Data Do? - a place for the data curious! Unleashing the power of data, one fascinating byte at a time.\nHi, I’m Lloyd Palum.\nMy students call me Prof. P., and with more than 30 years of experience in building data-intensive systems and teaching how to coax value from data at scale, I can help you crack the code and become more data-capable.\nLet’s unlock your data’s value together!"
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html",
    "title": "Data Intensive Applications - Introduction",
    "section": "",
    "text": "In the lecture, you’re introduced to the concept of data-intensive applications. These applications are designed to either augment or replace human capacity in various tasks. The essence of these applications lies in their ability to optimize specific outcomes, either by increasing or decreasing certain metrics.\nTo better understand the purpose and scope of data-intensive applications, consider the following matrix:\n\nAugmentation vs. Replacement: Data-intensive applications can be categorized based on whether they augment human capabilities or replace them entirely. Augmentation refers to enhancing human abilities, while replacement implies taking over tasks traditionally performed by humans.\nMore vs. Less: Another way to categorize these applications is based on their optimization goals. Are they designed to increase a certain metric or decrease it? For instance, an application might aim to increase safety or reduce costs.\n\n\n\n\nSelf-driving Cars: These vehicles are designed to replace human drivers. The primary goals are to increase safety (by reducing human errors) and decrease congestion (by optimizing routes and reducing unnecessary driving, such as searching for parking).\nMedical Image Analysis: In the medical field, data-intensive applications can augment doctors’ abilities. For instance, image analysis tools can help dermatologists identify and diagnose skin lesions more accurately.\nTransportation Route Planning: In the logistics and transportation sector, data-intensive applications can help in route planning. By optimizing routes, these applications can reduce fuel consumption and costs.\n\nRemember, the key to understanding data-intensive applications is to identify their purpose and the specific optimizations they aim to achieve. Whether it’s augmenting human abilities or replacing them, these applications are reshaping various industries and improving outcomes."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-definition-and-purpose",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-definition-and-purpose",
    "title": "Data Intensive Applications - Introduction",
    "section": "",
    "text": "In the lecture, you’re introduced to the concept of data-intensive applications. These applications are designed to either augment or replace human capacity in various tasks. The essence of these applications lies in their ability to optimize specific outcomes, either by increasing or decreasing certain metrics.\nTo better understand the purpose and scope of data-intensive applications, consider the following matrix:\n\nAugmentation vs. Replacement: Data-intensive applications can be categorized based on whether they augment human capabilities or replace them entirely. Augmentation refers to enhancing human abilities, while replacement implies taking over tasks traditionally performed by humans.\nMore vs. Less: Another way to categorize these applications is based on their optimization goals. Are they designed to increase a certain metric or decrease it? For instance, an application might aim to increase safety or reduce costs.\n\n\n\n\nSelf-driving Cars: These vehicles are designed to replace human drivers. The primary goals are to increase safety (by reducing human errors) and decrease congestion (by optimizing routes and reducing unnecessary driving, such as searching for parking).\nMedical Image Analysis: In the medical field, data-intensive applications can augment doctors’ abilities. For instance, image analysis tools can help dermatologists identify and diagnose skin lesions more accurately.\nTransportation Route Planning: In the logistics and transportation sector, data-intensive applications can help in route planning. By optimizing routes, these applications can reduce fuel consumption and costs.\n\nRemember, the key to understanding data-intensive applications is to identify their purpose and the specific optimizations they aim to achieve. Whether it’s augmenting human abilities or replacing them, these applications are reshaping various industries and improving outcomes."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#the-four-vs-of-big-data-and-compute-infrastructure",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#the-four-vs-of-big-data-and-compute-infrastructure",
    "title": "Data Intensive Applications - Introduction",
    "section": "The Four V’s of Big Data and Compute Infrastructure",
    "text": "The Four V’s of Big Data and Compute Infrastructure\nIn this section, you delve deeper into the foundational aspects of data-intensive applications, focusing on the characteristics of big data and the infrastructure that supports it.\n\nThe Four V’s of Big Data:\n\nVolume: Refers to the sheer amount of data being processed. As data-intensive applications handle vast amounts of information, understanding the scale becomes crucial.\nVariety: Highlights the different types of data sources and formats that applications might encounter. This could range from structured data in databases to unstructured data like images or videos.\nVelocity: Emphasizes the speed at which data is generated, processed, and made available. In real-time applications, the rate of data inflow and outflow can be staggering.\nVeracity: Points to the quality and trustworthiness of the data. Given that data comes from various sources, ensuring its accuracy and reliability is paramount.\n\n\n\nCompute Infrastructure:\nThe lecture provides a visual tour of Google’s data centers, showcasing the massive scale of infrastructure required to support data-intensive applications. Here’s what you should note:\n\nNetworking Room: This is where data requests are routed to the appropriate servers. It’s also the hub that allows data centers to communicate with each other globally.\nServer Floor: Houses powerful computers that handle a myriad of tasks, from processing search queries to hosting videos. Google has custom-designed these servers for compactness and energy efficiency.\nData Redundancy: To ensure data availability and safety, each piece of data is stored on at least two servers. Critical data is also backed up on digital tapes.\nCooling Systems: Given the heat generated by these servers, cooling towers are employed to maintain optimal temperatures.\n\n\n\nReal-world Example: Netflix on AWS\nNetflix, a leading entertainment service, offers a prime example of a data-intensive application operating at an immense scale. Here’s a snapshot:\n\nNetflix serves approximately 86 million customers across 190 countries.\nThey deliver about 150 million hours of streaming video daily.\nEverything runs out of Amazon Web Services (AWS), spread across three regions and 12 zones.\nAt peak times, Netflix operates around 100,000 compute instances.\n\nThis example underscores the vast compute resources required to run data-intensive applications and deliver seamless experiences to users worldwide.\nHaving explored the foundational aspects of big data and the infrastructure that supports it, you’re better equipped to appreciate the complexities and capabilities of data-intensive applications."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-in-real-world-scenarios",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-in-real-world-scenarios",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data-Intensive Applications in Real-World Scenarios",
    "text": "Data-Intensive Applications in Real-World Scenarios\nIn this section, you’re introduced to various real-world examples that demonstrate the power and impact of data-intensive applications. These examples provide a tangible understanding of how these applications are shaping industries, influencing decisions, and optimizing outcomes.\n\nSpotify Wrapped:\nSpotify, a popular music streaming service, offers a feature called “Wrapped” to its users. This feature provides a detailed review of a user’s listening habits over the year. It’s a nostalgic look back, showcasing top artists, genres, songs, and more. To generate this personalized review for millions of users, Spotify relies on a data-intensive application that processes vast amounts of user data. The sheer scale of this operation is a testament to the capabilities of data-intensive applications.\n\n\nUPS Route Optimization:\nUPS, a global logistics company, has developed a system called ORION (On-Road Integrated Optimization and Navigation). This system optimizes delivery routes for UPS drivers, leading to significant savings. By augmenting their operations with data-intensive applications, UPS has managed to save hundreds of millions of dollars annually. The application aims to use less (in terms of fuel, time, and resources) while augmenting the efficiency of delivery operations.\n\n\nNetflix Recommendation Engine:\nNetflix’s recommendation engine is a crucial component of its service. This engine suggests content to users based on their viewing habits, preferences, and other factors. As highlighted in the lecture, nearly 80% of the content viewed on Netflix is driven by these recommendations. This data-intensive application augments the user experience by presenting tailored content suggestions, leading to increased engagement and reduced churn.\n\n\nCOVID-19 Spread Modeling at Stanford:\nResearchers at Stanford developed a data-intensive application to model the spread of COVID-19. This application uses mobility data to simulate infection spread patterns. The primary insight was to utilize anonymized large-scale data from cell phones to understand mobility and contact patterns among individuals. The model covers ten of the largest metropolitan areas in the US, representing 98 million people. Policymakers can use this application to visualize the potential impact of reopening different sectors of the economy or regions, aiding in informed decision-making during the pandemic.\nThe video provides a brief overview of this research and its implications. By leveraging data-intensive applications, policymakers can make more informed decisions, balancing economic needs with public health concerns.\nThese real-world examples underscore the transformative potential of data-intensive applications. Whether it’s enhancing user experiences, optimizing operations, or aiding in critical decision-making, these applications are at the forefront of technological innovation."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-driven-policy-making-covid-19-spread-simulation",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-driven-policy-making-covid-19-spread-simulation",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data-Driven Policy Making: COVID-19 Spread Simulation",
    "text": "Data-Driven Policy Making: COVID-19 Spread Simulation\nIn this section, you’re introduced to a groundbreaking application developed by researchers at Stanford that uses data-intensive modeling to simulate the spread of COVID-19 based on various policy decisions.\n\nMobility Data and Infection Spread:\nThe researchers utilized anonymized large-scale data from cell phones to understand mobility patterns and contact patterns among individuals. This data provides insights into how people from different neighborhoods visit various points of interest, such as parks, grocery stores, schools, churches, etc. The duration of their stay and the area of these locations allow the researchers to compute the density of people in any given location at any given time.\n\n\nKey Findings:\n\nHigh-Risk Locations: The model revealed that just 10% of points of interest could account for over 80% of the infections in a city. Locations where infections are most likely to occur are places where people are densely packed for extended periods, such as restaurants, coffee shops, and fitness centers.\nDensity Caps and Economic Impact: Implementing density caps, where the occupancy of a location is capped at a percentage of its maximum, can significantly reduce infections. For instance, capping visits at 50% of maximum occupancy can reduce overall infections by over 50% while only causing a 5-10% reduction in visits to points of interest.\nAddressing Disparities: The research highlighted that low-income groups are more likely to visit places with higher densities. For example, grocery stores in low-income neighborhoods tend to be more crowded. Implementing density caps can help reduce disparities and ensure safer environments for all.\n\n\n\nReal-World Application:\nThe data-intensive application developed provides policymakers with a simulation tool that visualizes the potential impact of reopening different sectors or regions. By adjusting various parameters, policymakers can see the projected number of COVID-19 cases over time based on their decisions. This tool aids in balancing the economic needs with public health concerns, allowing for informed decision-making.\nThe underlying model for this application was built on vast amounts of mobility data, and the simulation tool serves as an augmentation for policymakers, offering predictive insights into the consequences of their decisions.\nHaving explored this innovative application that combines data science with public health policy, you gain a deeper appreciation for the transformative potential of data-intensive applications in addressing real-world challenges."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#fuel-optimization-in-trucking-vnomics-true-fuel-network",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#fuel-optimization-in-trucking-vnomics-true-fuel-network",
    "title": "Data Intensive Applications - Introduction",
    "section": "Fuel Optimization in Trucking: Vnomics True Fuel Network",
    "text": "Fuel Optimization in Trucking: Vnomics True Fuel Network\nIn this section, you’re introduced to the challenges and opportunities associated with fuel optimization in the trucking industry. The lecture emphasizes the significance of fuel consumption in trucking operations and how data-intensive applications can play a pivotal role in optimizing fuel usage.\n\nThe Scale of the Trucking Industry:\n\nThe trucking industry in the United States spends approximately $150 to $160 billion annually on diesel fuel.\nOn average, a truck consumes about $50,000 worth of diesel every year.\nThere’s roughly one truck for every 100 people in the U.S., translating to millions of trucks on the road.\nAlmost everything around us, from the food we eat to the products we buy, has been transported by these trucks at some point.\n\n\n\nThe Optimization Opportunity:\nGiven the vast amount of fuel consumed by the trucking industry, there’s a significant opportunity for optimization. The goal is to use less fuel, which not only has economic benefits but also contributes to sustainability and reduces environmental impact.\n\n\nVnomics True Fuel Network:\nVnomics, a tech company, has developed a product called the True Fuel Network. This data-intensive application aims to optimize fuel consumption in trucking operations. The product processes millions of trips, providing insights into fuel consumption patterns and offering actionable recommendations.\nKey Features:\n\nDriver Behavior Analysis: Driver behavior can influence fuel efficiency by up to 30%. The True Fuel Network provides real-time in-cab coaching, offering audible tones when drivers exhibit inefficient behaviors. This immediate feedback helps drivers adjust their driving habits to optimize fuel consumption.\nTrip Context Analysis: Factors such as the load carried, the type of truck, routing, and environmental conditions (like weather and terrain) play a crucial role in fuel consumption. The True Fuel Network analyzes all these factors to provide comprehensive insights.\nVisual Data Representation: The lecture showcases a visual representation of the trips processed by the True Fuel Network. This visualization highlights trucking routes, urban hubs, and key trucking centers across the country.\nAutomated Reporting: The system offers automated reporting features, sending weekly reports to driver trainers and management. These reports provide insights into fleet performance, highlighting areas for improvement and coaching opportunities.\n\n\n\nReal-World Application:\nThe lecture showcases a video that further elaborates on the features and benefits of the True Fuel Network. The video emphasizes the importance of real-time coaching, driver feedback mechanisms, and the potential savings that can be achieved through optimized fuel consumption.\nIn essence, the Vnomics True Fuel Network serves as a testament to the transformative potential of data-intensive applications in the trucking industry. By leveraging data and technology, the industry can achieve significant savings, reduce its environmental footprint, and enhance operational efficiency.\n\n\n12 Steps to Applied AI - A roadmap for every machine learning project\nThe article titled “12 Steps to Applied AI” provides a comprehensive roadmap for executing machine learning and artificial intelligence projects. The author emphasizes the importance of following a structured approach to ensure the success of such projects. Here’s a summarized breakdown of the 12 steps:\n\nReality Check and Setup: Before diving into ML/AI, ensure that there’s a genuine need for it. Verify that you have the necessary data and hardware.\nDefine Your Objectives: Clearly articulate the goals and success metrics for your project.\nGet Access to Data: Establish processes for collecting relevant data, including labels for supervised learning.\nSplit Your Data: Divide your data into training, validation, and test sets to ensure unbiased model evaluation.\nExplore Your Data: Analyze your training data, perform sanity checks, and engineer new features.\nPrepare Your Tools: Familiarize yourself with ML/AI tools and ensure your data is in the right format.\nTrain Some Models: Use algorithms to find patterns in your data and create predictive models.\nDebug, Analyze, and Tune: Dive deep into your model’s performance and make necessary adjustments.\nValidate Your Models: Use the validation dataset to assess your model’s performance without overfitting.\nTest Your Model: Evaluate your model’s performance on entirely new data to ensure its real-world applicability.\nProductionize Your System: Convert your prototype into a production-ready system, ensuring it integrates well with existing systems.\nRun Live Experiments: Before full deployment, test your model in a live environment to ensure its effectiveness.\nMonitor and Maintain: Continuously monitor your deployed system and make necessary updates to maintain its reliability.\n\nThe author emphasizes that the art of applying AI to solve business problems is not just about algorithms but involves asking the right questions, preparing data, finding patterns, validating results, and ensuring long-term reliability."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#designing-data-intensive-applications-the-drivetrain-approach",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#designing-data-intensive-applications-the-drivetrain-approach",
    "title": "Data Intensive Applications - Introduction",
    "section": "Designing Data-Intensive Applications: The Drivetrain Approach",
    "text": "Designing Data-Intensive Applications: The Drivetrain Approach\nIn this section, you’re introduced to a systematic approach to designing data-intensive applications, known as the Drivetrain Approach. This methodology emphasizes the importance of starting with a clear objective and then leveraging data and models to achieve that goal.\n\nThe Four Steps of the Drivetrain Approach:\n\nObjective Definition: Begin by clearly defining the goal of your application. This is the foundation upon which everything else is built. You need to understand whether you’re aiming to augment human capacity or replace it. Additionally, determine the basis of the optimization: are you aiming for more or less of a particular metric?\nLeverage Data: Once the objective is clear, the next step is to identify the data that can help achieve this goal. This is where you determine what kind of data is relevant and how it can be used to influence the desired outcome.\nModel Development: With the relevant data in hand, you can now develop models that simulate various scenarios. These models allow for exploration and “what-if” analyses, enabling you to predict potential outcomes based on different inputs.\nOutcome Optimization: After simulating various scenarios, you analyze the outputs to determine the best course of action. This step involves fine-tuning and iterating on the model to optimize the desired outcome.\n\n\n\nReal-World Application: COVID Predictions\nThe lecture revisits the COVID-19 spread simulation developed by Stanford researchers. This application serves as a practical example of the Drivetrain Approach:\n\nObjective: The goal was to predict the spread of COVID-19 based on different policy decisions, balancing economic needs with public health concerns.\nLeverage Data: Researchers utilized anonymized large-scale mobility data from cell phones to understand how people move and interact.\nModel Development: Using the mobility data, a model was developed to simulate the spread of the virus based on various factors, such as opening or closing certain points of interest.\nOutcome Optimization: Policymakers can use the simulation tool to visualize the potential impact of their decisions, allowing them to make informed choices that optimize public health outcomes.\n\n\n\nJeremy Howard and the Drivetrain Metaphor:\nJeremy Howard, a notable figure in the data science community, is highlighted for his contributions to this approach. With a rich background, including being a Kaggle Grand Master and co-founding Fast.ai, Howard has successfully applied the Drivetrain Approach in various ventures, from insurance pricing optimization to medical analytics based on genomic information.\nIn essence, the Drivetrain Approach offers a structured methodology to design and implement data-intensive applications. By starting with a clear objective and systematically leveraging data and models, you can optimize outcomes and achieve tangible results."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#simulation-optimization-and-validation-in-data-intensive-applications",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#simulation-optimization-and-validation-in-data-intensive-applications",
    "title": "Data Intensive Applications - Introduction",
    "section": "Simulation, Optimization, and Validation in Data-Intensive Applications",
    "text": "Simulation, Optimization, and Validation in Data-Intensive Applications\nIn this section, you’re taken through the intricacies of simulation, optimization, and the importance of validation in data-intensive applications. The lecture emphasizes the need to go beyond just modeling and to actively simulate and optimize outcomes for actionable insights.\n\nSimulation and Real-Time Feedback:\nThe lecture delves into the True Fuel application, which provides real-time feedback to drivers based on their driving behavior. The application doesn’t just rely on a static model; instead, it actively simulates optimal driving behavior in real-time and compares it to the actual behavior of the driver.\n\nReal-Time Modeling: As the driver operates the vehicle, the application continuously models the optimal operation of the vehicle.\nAudible Feedback: If the driver’s behavior deviates significantly from the optimal operation, the application provides audible feedback, much like a rumble strip on a road. This immediate feedback helps drivers adjust their behavior to optimize fuel consumption.\n\n\n\nThe Importance of What-If Analysis:\nA critical aspect of data-intensive applications is the ability to perform “what-if” analyses. This involves simulating various scenarios to understand potential outcomes. It’s not just about predicting what did happen but more about predicting what could happen under different circumstances.\n\nCounterfactual Simulation: The goal is to simulate outcomes based on different inputs or conditions. For instance, in the context of the COVID-19 spread simulation, it’s about predicting the spread based on different policy decisions.\nOptimization: After simulating various scenarios, the next step is to analyze the results and determine the best course of action. This involves identifying the optimal outcomes based on the desired objectives.\n\n\n\nValidation in Machine Learning:\nTowards the end of this section, there’s a discussion about the validation process in machine learning. Validation is crucial to ensure that the models developed are accurate and reliable.\n\nTesting Data: Typically, when building a machine learning model, a portion of the data is set aside for testing. This data is used to evaluate the performance of the model and ensure it’s making accurate predictions.\nClassifier Validation: The lecture touches upon the concept of classifier validation, emphasizing the need to rigorously test and validate machine learning classifiers to ensure they’re making accurate classifications.\n\nThe lecture concludes with a Q&A session where students raise questions about the validation process and its importance in data-intensive application development.\nUnderstanding simulation, optimization, and validation is crucial when designing data-intensive applications. These processes ensure that the applications not only make accurate predictions but also provide actionable insights that can drive decision-making."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#challenges-and-considerations-in-data-intensive-applications",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#challenges-and-considerations-in-data-intensive-applications",
    "title": "Data Intensive Applications - Introduction",
    "section": "Challenges and Considerations in Data-Intensive Applications",
    "text": "Challenges and Considerations in Data-Intensive Applications\nIn this section, you’re introduced to the complexities and challenges associated with building and maintaining data-intensive applications. The lecture emphasizes the dynamic nature of these applications and the need for continuous monitoring and updating.\n\nProbabilistic Nature:\nData-intensive applications are inherently probabilistic, not deterministic. This means that while they can make predictions or recommendations based on data, there’s always an element of uncertainty. It’s crucial to understand and account for this uncertainty when designing and using these applications.\n\n\nContinuous Updating:\nOne of the significant challenges with data-intensive applications is the need for continuous updating. As new data becomes available or as the environment changes, the models underpinning these applications might become outdated or less accurate.\n\nDynamic Data Environment: The data feeding into these applications can change over time. For instance, user behaviors might evolve, or new types of data might become available. This dynamic nature requires the models to be adaptable.\nModel Drift: Over time, a model that was once accurate might start to drift from reality. This drift can be due to various factors, including changes in the data distribution or external factors affecting the system.\n\n\n\nThe Autonomous Vehicle Example:\nThe lecture revisits the example of autonomous vehicles to highlight the challenges in building data-intensive applications.\n\nMultiple Models: An autonomous vehicle doesn’t rely on just one model. Instead, it uses an ensemble of models to handle different tasks, from navigation to object detection.\nSimulation and Testing: Before deploying these models in real-world scenarios, they undergo rigorous simulation and testing. This process helps identify potential issues and optimize the models for better performance.\nOptimization Challenges: With so many models working in tandem, optimization becomes a complex task. The goal is to ensure that all models work harmoniously to drive the vehicle safely and efficiently.\n\n\n\nEmphasis on Continuous Monitoring:\nGiven the challenges and the dynamic nature of data-intensive applications, there’s a strong emphasis on continuous monitoring. Regularly monitoring the performance of these applications can help identify issues early on and ensure that they continue to deliver accurate and reliable results.\nIn essence, building and maintaining data-intensive applications is a continuous process. It requires a deep understanding of the data, the models, and the environment in which these applications operate. Regular updates, monitoring, and optimization are crucial to ensure their long-term success and reliability."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-a-team-sport",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-intensive-applications-a-team-sport",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data-Intensive Applications: A Team Sport",
    "text": "Data-Intensive Applications: A Team Sport\nIn this section, you’re introduced to the collaborative nature of building data-intensive applications. The lecture emphasizes that creating these applications is not a solo endeavor but rather a team sport, requiring collaboration across various roles and skill sets.\n\nRoles in Building Data-Intensive Applications:\n\nData Scientists: Trained in the art and science of extracting insights from data, data scientists bring a strong background in modeling, analysis, and interpretation. They are equipped with the tools and techniques to dive deep into data and derive meaningful patterns.\nData Analysts: These professionals focus more on mining data, programming for visualization, and communicating findings. They are adept at quickly assessing data quality and generating hypotheses based on initial explorations. Data analysts often serve as the bridge between raw data and actionable insights, providing the initial spark of inspiration.\nOther Roles: While the lecture primarily highlights data scientists and data analysts, it’s implied that there are other roles involved, such as data engineers, business analysts, and domain experts. Each role brings a unique perspective and set of skills to the table.\n\n\n\nKey Considerations:\n\nUnderstanding Role Requirements: As you embark on a career in the commercial sector, it’s crucial to understand the specific requirements of roles labeled as “data scientist.” Some companies might use the term “data scientist” when they are actually looking for a data analyst. It’s essential to clarify role expectations and responsibilities during the hiring process.\nCollaboration is Key: Building data-intensive applications requires collaboration across various roles. Each role complements the others, ensuring that the application is robust, accurate, and delivers value. Data scientists might focus on modeling, while data analysts might emphasize data quality and initial insights. Together, they ensure the application’s success.\nEthics and Regulations: As data-intensive applications become more prevalent, there’s a growing emphasis on ethics and regulations. Ensuring transparency in modeling and optimization processes is becoming increasingly important. As a professional in this field, you’ll need to be aware of ethical considerations and potential regulatory requirements.\n\nIn essence, building data-intensive applications is a collaborative endeavor. It requires a harmonious blend of skills, expertise, and perspectives. As you navigate the world of data science and application development, remember that it’s a team sport, and collaboration is at its heart."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#building-data-intensive-applications-a-hierarchical-approach",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#building-data-intensive-applications-a-hierarchical-approach",
    "title": "Data Intensive Applications - Introduction",
    "section": "Building Data-Intensive Applications: A Hierarchical Approach",
    "text": "Building Data-Intensive Applications: A Hierarchical Approach\nIn this section, you’re introduced to a hierarchical approach to building data-intensive applications. This pyramid model provides a structured way to think about the various stages and activities involved in developing these applications.\n\nThe Pyramid Model:\nThe pyramid is composed of different layers, each representing a specific set of activities. As you work your way from the bottom to the top of the pyramid, the activities become more refined and specific.\n\nFoundation: At the base of the pyramid, you have foundational activities. These are the initial steps that set the stage for the entire application development process. It involves understanding the business context, defining objectives, and gathering relevant data.\nData Preparation: As you move up the pyramid, the next layer focuses on data preparation. This involves cleaning the data, handling missing values, and transforming the data into a format suitable for modeling.\nModeling: The middle layer of the pyramid is dedicated to modeling. Here, you leverage various algorithms and techniques to develop models that can make predictions or provide insights based on the data.\nEvaluation: Once the models are developed, the next step is evaluation. This layer emphasizes the importance of rigorously testing and validating the models to ensure they are accurate and reliable.\nDeployment: At the top of the pyramid, you have deployment. This is where the models are integrated into real-world applications and systems, making them accessible to end-users.\n\n\n\nTeam Collaboration:\nAs you navigate through the pyramid, it’s essential to remember the collaborative nature of building data-intensive applications. Different roles, such as data scientists, data analysts, and data engineers, come into play at various stages of the pyramid.\n\nRole Dynamics: While data scientists might be heavily involved in the modeling layer, data engineers play a crucial role in the foundational and data preparation layers. Understanding these dynamics is vital for effective collaboration.\nIterative Process: The pyramid model also emphasizes the iterative nature of building data-intensive applications. Often, you might need to loop back to previous layers to refine the data or models based on new insights or changes in the business context.\n\nIn essence, the pyramid model offers a structured and systematic approach to building data-intensive applications. By understanding the various layers and activities, you can ensure a more streamlined and effective development process."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#setting-up-for-success-in-data-intensive-applications",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#setting-up-for-success-in-data-intensive-applications",
    "title": "Data Intensive Applications - Introduction",
    "section": "Setting Up for Success in Data-Intensive Applications",
    "text": "Setting Up for Success in Data-Intensive Applications\nIn this section, you’re introduced to the foundational principles and considerations that set the stage for successful data-intensive application development. The lecture emphasizes the importance of understanding the business context, ensuring the relevance of the application’s outputs, and the role of early data exploration.\n\nUnderstanding the Business Context:\n\nLeadership’s Role: It’s crucial for the person or team leading a data-intensive application project to have a deep understanding of the business. This ensures that the application aligns with the business’s objectives and can deliver actionable insights that fit within the broader business context.\nAvoiding Misalignment: The outputs of a data-intensive application should be relevant and actionable. It’s essential to avoid confusing the end goal with the means to achieve it. For instance, while a data science team might focus on metrics like accuracy or loss factors, these might not always be directly relevant to business stakeholders. The application’s outputs should be tailored to the audience and the business’s needs.\n\n\n\nEarly Data Exploration:\n\nDive into Raw Data: Before committing to a specific approach or model, it’s beneficial to delve into the raw data. This exploration can provide insights into potential directions for the application.\nRole of Data Analysts: Data analysts can play a pivotal role in this early exploration phase. By slicing, dicing, and visualizing the data, they can uncover patterns and insights that can guide the application’s development.\nFinding Inspiration: The initial exploration phase is about finding inspiration in the data. It’s about ensuring that the data supports the objectives of the application and that there’s a clear direction to pursue.\n\n\n\nEnsuring Relevance and Actionability:\n\nTailored Outputs: The outputs of a data-intensive application should be tailored to the audience. Whether it’s business stakeholders, end-users, or other teams, the information presented should be relevant, understandable, and actionable.\nIterative Process: Building a data-intensive application is an iterative process. It’s essential to continuously refine the application based on feedback, new data, and changing business needs.\n\nThe lecture references Cassie Kozyrkov, the Head of Decision Science at Google, who emphasizes the importance of setting up for success in data-intensive application development. Her insights underscore the need for a deep understanding of the business, early data exploration, and ensuring the relevance of the application’s outputs.\nIn essence, setting up for success in data-intensive applications requires a combination of business understanding, data exploration, and continuous refinement. By keeping these principles in mind, you can ensure that the application delivers value and aligns with the broader business objectives."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#prerequisites-for-data-intensive-applications",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#prerequisites-for-data-intensive-applications",
    "title": "Data Intensive Applications - Introduction",
    "section": "Prerequisites for Data-Intensive Applications",
    "text": "Prerequisites for Data-Intensive Applications\nIn this section, you’re introduced to the prerequisites and foundational considerations that need to be addressed before diving into the development of data-intensive applications. The lecture emphasizes the importance of having a clear understanding of the business context, ensuring the availability of relevant data, and assembling the right team with the necessary skill sets.\n\nKey Considerations:\n\nTraditional Software vs. ML/AI Approach: Before embarking on a data-intensive application project, it’s essential to evaluate whether a traditional software approach might suffice or if a more complex ML or AI-based modeling approach is necessary. Sometimes, simple heuristic methods (like “if-then-else” logic) can achieve the desired results without the overhead of a data-intensive application.\nUser Experience (UX): Beyond just producing actionable results, it’s crucial to ensure that these results are presented in a user-friendly manner. This involves considering how the outputs will be displayed, ensuring they are easily understandable, and making them accessible when needed.\nEthical Development: As data-intensive applications become more prevalent, there’s an increasing emphasis on ethical considerations. It’s essential to ensure that the applications are developed with transparency and that their recommendations or actions align with ethical standards.\nSetting Reasonable Expectations: Define what constitutes success for the application. Understand the potential impact of occasional mistakes. For instance, while an error in a movie recommendation system might be inconsequential, a mistake in a self-driving car system could be catastrophic.\nResource Availability: Consider the availability of cloud resources or other computational capacities. Some applications, like streaming services, require massive computational power, which might only be feasible with cloud resources.\nData Availability: Data is the backbone of these applications. Ensure that there’s enough relevant data available to train and refine the models. Without sufficient data, the application might not perform optimally.\nTeam Composition: Building data-intensive applications requires a diverse set of skills. It’s essential to have a team that encompasses all the necessary expertise, from data science to software development and domain knowledge.\n\n\n\nReflecting on the Prerequisites:\nThe lecture references the term “unicorn” to describe individuals who possess all the skills necessary to build a data-intensive application from scratch. Such individuals are rare, emphasizing the importance of collaborative efforts and team dynamics in the development process.\nIn essence, before diving into the development of a data-intensive application, it’s crucial to address these foundational considerations. By ensuring that the business context is clear, the necessary data and resources are available, and the right team is in place, you set the stage for a successful application development journey."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#key-takeaways-and-concluding-remarks",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#key-takeaways-and-concluding-remarks",
    "title": "Data Intensive Applications - Introduction",
    "section": "Key Takeaways and Concluding Remarks",
    "text": "Key Takeaways and Concluding Remarks\nIn this concluding section, the lecture revisits the main points discussed throughout the presentation and emphasizes the importance of a structured approach to building data-intensive applications.\n\nRecap:\n\nUnderstanding Data-Intensive Applications: The lecture began by defining what a data-intensive application is, emphasizing its role in augmenting or replacing human capacity and its optimization potential.\nFour Quadrant Metaphor: A metaphor was introduced to categorize data-intensive applications based on augmentation vs. replacement and optimization in terms of more vs. less. This framework provides a quick way to understand the primary function of an application.\nPrerequisites for Success: Before diving into the development of a data-intensive application, several prerequisites were highlighted. These include understanding the business context, ensuring data availability, assembling the right team, and setting clear performance criteria.\nMinimum Performance Criteria: One of the most emphasized points was the need to set a minimum performance criteria before starting the development process. This criteria serves as a benchmark to ensure that the application meets the desired standards. It’s crucial to be stringent about this criteria and be willing to halt the project if it’s not met.\nEthical Considerations: The lecture touched upon the growing importance of ethical considerations in data-intensive application development. Ensuring transparency, fairness, and ethical decision-making is paramount.\nInterpretability and Metrics: The importance of model interpretability was highlighted, emphasizing the need to understand why a model makes certain decisions. Additionally, setting clear metrics for success and regularly reviewing them is crucial for the ongoing success of the application.\n\n\n\nConcluding Remarks:\nThe lecture concludes with a call to action for aspiring data scientists and professionals in the field. Building data-intensive applications is a complex yet rewarding endeavor. It requires a combination of technical expertise, business acumen, and a collaborative spirit. By approaching the process with rigor, clarity, and a commitment to ethical considerations, you can develop applications that not only deliver value but also stand the test of time.\nThe provided links and resources throughout the lecture serve as additional tools for deepening your understanding and further exploration."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#introduction-and-context-for-the-course",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#introduction-and-context-for-the-course",
    "title": "Data Intensive Applications - Introduction",
    "section": "Introduction and Context for the Course",
    "text": "Introduction and Context for the Course\nThis visual is a picture that encapsulates the essence of what the course aims to teach. The goal for the evening is to transition from understanding what a data-intensive application is, which was the focus of the last lecture, to diving deep into how these applications are developed.\n\n\n\nDIA Context\n\n\nThe challenges that arise during the development of these applications are highlighted. You’re made aware that the journey will not be straightforward. There are numerous activities and intricacies involved, and the lecture aims to shed light on these aspects. The speaker emphasizes the top line of the visual, which represents various activities associated with building data-intensive applications. These activities range from ingesting data from files, databases, and streams to preparing the data, building models, and deploying and monitoring those models at scale.\nAs you delve deeper into the lecture, you realize that the course will equip you with the knowledge of various platform pieces. These tools are not just for building the applications but also for navigating the challenges and pitfalls that might come your way."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#steps-in-building-data-intensive-applications",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#steps-in-building-data-intensive-applications",
    "title": "Data Intensive Applications - Introduction",
    "section": "Steps in Building Data-Intensive Applications",
    "text": "Steps in Building Data-Intensive Applications\nThe narrative shifts to a more structured approach, outlining the steps involved in building data-intensive applications:\n\nSetting Your Objective: This is a crucial first step. Before diving into the technicalities, you need to have a clear goal in mind. This step is about addressing various questions upfront, ensuring you have a clear direction before embarking on the journey.\nGathering Data: Once your objective is clear, the next step is to gather relevant data. This is the foundation upon which your application will be built.\nExploratory Data Analysis (EDA): With the data in hand, you embark on a journey of exploration. EDA allows you to develop insights and understand the nuances of the data you’ve gathered.\nData Curation and Feature Development: This step emphasizes the importance of data quality. You’re reminded of the term ‘veracity’ from the last lecture, which relates to the trustworthiness of data sources. Data curation ensures that you have control over the quality of your data. Alongside this, you also delve into feature development, an aspect you might already be familiar with.\nTooling and Configuration of Development: The lecture touches upon the tools and configurations essential for development. Interestingly, you learn that the machine learning component, which might seem central, often diminishes in importance in the broader context of these applications.\nModel Training at Scale: While you might be familiar with model training, the lecture emphasizes doing it at scale and in a controlled manner. It’s about capturing details of the lifecycle, ensuring you don’t lose track of your progress.\nDeployment and Maintenance: Once your application is ready, the final step is to deploy and maintain it. This ensures that your application remains relevant and functional in the real world."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#setting-your-objective",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#setting-your-objective",
    "title": "Data Intensive Applications - Introduction",
    "section": "Setting Your Objective",
    "text": "Setting Your Objective\nYou’re reminded of the importance of starting with a clear objective in mind. Before diving into the technicalities of building a data-intensive application, it’s essential to understand the problem you’re trying to solve. Is it a customer’s problem? A business challenge? The lecture emphasizes the importance of optimization and ensuring that you have a clear metric aligned with your objective.\nBuilding a data-intensive application isn’t a solo endeavor. It’s a team sport. Different individuals, each with their expertise, come together to achieve a common goal. At the forefront of setting the objective are the business leaders and domain experts. They ensure that the high-level decisions about the application align with the domain’s nuances and the business’s needs."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#gathering-your-data",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#gathering-your-data",
    "title": "Data Intensive Applications - Introduction",
    "section": "Gathering Your Data",
    "text": "Gathering Your Data\nAs you move to the next step, the focus shifts to gathering data. The domain experts remain involved, ensuring that the data aligns with the application’s objective. However, data engineers now come into play. They are responsible for ingesting data at scale, directing it towards a ‘lake house’ (a unified platform that combines the capabilities of data lakes and data warehouses). The emphasis is on the veracity of the data – its quality, source, and relevance.\nData governance becomes a focal point. In an era where privacy and data protection are paramount, ensuring ethical treatment of data is crucial. The lecture underscores the importance of having control over where the data comes from and how it’s used."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#exploratory-data-analysis-eda",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#exploratory-data-analysis-eda",
    "title": "Data Intensive Applications - Introduction",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nWith the data in hand, the next step is EDA. Domain experts continue to be involved, ensuring that the data analysis aligns with the application’s objectives. Data analysts join the fray, visualizing the data and ensuring it provides the necessary leverage for the application.\nThe lecture introduces you to the science of data analysis and EDA. A resource from Google, focusing on the analysis of logs crucial for their search capabilities, is highlighted. This resource provides practical advice for analyzing large and complex datasets."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-curation",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-curation",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data Curation",
    "text": "Data Curation\nThe narrative now shifts to data curation. Data engineers play a pivotal role in this step, focusing on creating what the lecture terms as a ‘silver dataset’. This dataset is one over which there’s significant control, setting the stage for effective trade-offs between bias and variance in the modeling lifecycle.\nThis curated dataset is not just for training models. Once a model is deployed, this dataset will also provide the features necessary for the model to generalize in real-time. The lecture emphasizes the dual role of this dataset – for historical data capture and runtime data capture during inference.\nThe data curation step also touches upon the importance of splitting the data for training and testing. Another resource is provided, offering deeper insights into this aspect."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-validation",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-validation",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data Validation",
    "text": "Data Validation\nIn this section, the lecture emphasizes the significance of data validation in machine learning. The process is crucial at two different points in time: during the preparation for model training and during runtime when the model is in inference mode.\n\nPre-training Validation\nBefore training the model, a critical evaluation of the data is necessary. You are guided to look for anomalies and outliers in the data, filtering out invalid data points based on domain expertise. This step ensures that the data used for training the model is of high quality and relevance.\n\n\nRuntime Validation\nDuring runtime, when new data is ingested for model inference, validation remains a key aspect. The lecture stresses the importance of inter-batch validation, ensuring that the characteristics of the data remain consistent with what the model was trained on. This involves checking whether the data profiles, distributions, and other characteristics remain valid during the model’s operational phase."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#transition-to-machine-learning-model-development",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#transition-to-machine-learning-model-development",
    "title": "Data Intensive Applications - Introduction",
    "section": "Transition to Machine Learning Model Development",
    "text": "Transition to Machine Learning Model Development\nMoving forward, the lecture navigates towards the development of machine learning models. A concept introduced is the “hidden technical debt in machine learning systems,” which will be elaborated on later in the course.\n\nStarting Simple\nAn advice shared is the benefit of starting simple in model development. Initially, using simpler models like logistic regression or simple regression models can be advantageous. It allows for easier and quicker iterations, enabling you to pay more attention to the overall system and capture essential details effectively. Over time, as the need arises, the models can be made more sophisticated to improve performance and meet specific goals.\n\n\nModel Life Cycle\nEntering the model life cycle, the involvement of machine learning engineers and data analysts becomes prominent. The focus is on scrutinizing the training process, identifying problematic elements or feature vectors that the model may not be generalizing well on. Iteration becomes a central theme, emphasizing the need for continuous improvement and tuning, such as hyperparameter tuning, to enhance the model’s performance."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-tuning-and-debugging",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-tuning-and-debugging",
    "title": "Data Intensive Applications - Introduction",
    "section": "Model Tuning and Debugging",
    "text": "Model Tuning and Debugging\nThe lecture delves deeper into the intricacies of tuning and debugging models. While you might already be familiar with the machine learning life cycle, the emphasis here is on doing this at scale and in a controlled manner. Tools like Spark and MLflow are introduced, highlighting their benefits for large-scale operations and maintaining control over the machine learning process."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-validation",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-validation",
    "title": "Data Intensive Applications - Introduction",
    "section": "Model Validation",
    "text": "Model Validation\nValidation is the step where data analysts play a pivotal role. It ensures that the model generalizes well on a validation set. The lecture reiterates the importance of not “peeking” into the validation set and ensuring that there’s no data leakage from the test set back into the model development."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-testing",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#model-testing",
    "title": "Data Intensive Applications - Introduction",
    "section": "Model Testing",
    "text": "Model Testing\nThis is the culmination of all the previous steps. Decision-makers and data scientists come together to critically evaluate the model’s performance against the test set. This is the “honest broker” part of the process, where the model’s adherence to sound statistical principles is evaluated. If the model fails this test, the process goes back to the data gathering stage. It’s emphasized that this test is where the “rubber meets the road.”"
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#deployment-and-mlops",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#deployment-and-mlops",
    "title": "Data Intensive Applications - Introduction",
    "section": "Deployment and MLOps",
    "text": "Deployment and MLOps\nOnce the model passes the test, the next step is deployment. The lecture transitions from traditional data analysis and machine learning to a more software engineering-oriented approach. A model is described as a “recipe” that needs to be captured for auto deployments. The challenge of MLOps (Machine Learning Operations) is introduced, emphasizing the need for version control over data, code, and the model itself. This trio makes MLOps distinct from traditional software DevOps and potentially more complex."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#launching-and-continuous-improvement",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#launching-and-continuous-improvement",
    "title": "Data Intensive Applications - Introduction",
    "section": "Launching and Continuous Improvement",
    "text": "Launching and Continuous Improvement\nThe final stages involve launching the model and ensuring its continuous improvement. A/B testing is introduced as a method to compare the new machine learning system against traditional heuristic-based systems. By running both systems side by side in real-world scenarios, their performance can be benchmarked against each other.\nEven after the machine learning system is deployed, the work doesn’t stop. The lecture emphasizes the need for continuous tuning and adjustments, especially as datasets evolve and change over time."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#monitoring-and-maintaining",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#monitoring-and-maintaining",
    "title": "Data Intensive Applications - Introduction",
    "section": "Monitoring and Maintaining",
    "text": "Monitoring and Maintaining\nOnce the model is deployed and running, the work doesn’t end. The lecture emphasizes the continuous need for monitoring and maintenance. Data analysts and reliability engineers play a pivotal role in this phase. Reliability engineers, typically software-centric individuals, work alongside data analysts to ensure the model’s consistent performance.\nThe cost of monitoring and maintaining a model is highlighted. While building and deploying a machine learning system might be relatively cheap, especially with the advent of cloud computing and vast data availability, maintaining it over time can be expensive. It’s crucial to track whether the model remains within specified parameters. One of the tasks you might undertake is building a simple monitoring system to observe the model’s performance in real-time. This system would help identify any drift in the model’s performance, potentially triggering a retraining phase."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#technical-debt-in-machine-learning",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#technical-debt-in-machine-learning",
    "title": "Data Intensive Applications - Introduction",
    "section": "Technical Debt in Machine Learning",
    "text": "Technical Debt in Machine Learning\n\n\n\nTech Debt in ML Systems\n\n\nThe lecture introduces the concept of “technical debt” in machine learning. While it might be easy to start building a machine learning system, taking shortcuts without considering the long-term implications can lead to accumulating technical debt. This debt refers to the future costs of rectifying shortcuts and oversights made during the initial development phase.\nIf not addressed, technical debt can lead to significant challenges in the future, making the system expensive to maintain and potentially leading to its failure. The metaphor of going “bankrupt” is used to describe a scenario where a system becomes too cumbersome to manage due to accumulated technical debt.\nA key takeaway is that the actual machine learning coding, which many might be familiar with, becomes a small part of the entire process of building and maintaining data-intensive applications. The lecture references a paper that delves deeper into this concept, highlighting the various components involved in building these applications beyond just the machine learning code."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#components-of-a-viable-machine-learning-system",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#components-of-a-viable-machine-learning-system",
    "title": "Data Intensive Applications - Introduction",
    "section": "Components of a Viable Machine Learning System",
    "text": "Components of a Viable Machine Learning System\nThe lecture dives deeper into the various components that make up a comprehensive machine learning system. The scale of these blocks is designed to emphasize the relative effort required for each component, with the actual machine learning coding being a smaller part compared to other aspects.\n\nConfiguration\nConfiguration management is crucial. It involves tracking the versions of models in production, the features used, and other runtime configurations. Centralizing and versioning configurations ensures consistency and clarity.\n\n\nData Collection and Feature Extraction\nData collection and feature extraction are foundational to any machine learning system. The lecture introduces the concept of bronze, silver, and gold data sources, emphasizing the importance of having control over data collection and feature extraction. Technologies like Data Lakes and Delta Lake play a pivotal role in this aspect.\n\n\nMachine Learning Core\nWhile a smaller component, the machine learning core is where the actual model development happens. This encompasses tools and frameworks like TensorFlow, PyTorch, and Scikit-learn.\n\n\nResource Management\nWhen scaling machine learning operations, resource management becomes vital. Distributed compute management engines like Kubernetes and TensorFlow’s serving component are examples of tools that handle large-scale operations.\n\n\nAnalysis Tools\nVarious tools facilitate the analysis and profiling of data and models. TensorFlow Data Validation, Pandas library, TensorBoard, and Facets are some of the tools mentioned that aid in exploratory data analysis and model evaluation.\n\n\nProcess Management\nMLflow is highlighted as a popular tool in the industry for tracking, registering, and packaging models. It provides an agnostic approach, supporting various machine learning frameworks.\n\n\nServing Infrastructure\nThis component deals with the deployment and runtime management of models. It ensures that models are accessible and can handle requests efficiently.\n\n\nMonitoring\nMonitoring is essential to ensure that models perform as expected in real-time scenarios. Tools within TensorFlow’s technology stack, such as data validation tools, are introduced for large-scale monitoring.\nThe lecture emphasizes that while the machine learning code is essential, it’s just one part of a broader system. Ensuring that all these components work harmoniously is crucial to avoid accumulating technical debt and to build robust, scalable machine learning systems."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#entanglement-and-abstraction-boundaries",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#entanglement-and-abstraction-boundaries",
    "title": "Data Intensive Applications - Introduction",
    "section": "Entanglement and Abstraction Boundaries",
    "text": "Entanglement and Abstraction Boundaries\nThe lecture delves into the complexities of machine learning systems, particularly the challenges posed by entanglement and abstraction boundaries. When using machine learning to solve problems, especially those that are difficult to express with traditional software, you’re introduced to the concept of entanglement. This refers to the interconnectedness of various components and the potential for changes in one area to affect others.\n\nThe Challenge of Entanglement\nEntanglement arises due to the inherent nature of machine learning. For instance, if you’ve built a model based on a specific data distribution and that distribution changes, it can affect the entire system. This interconnectedness means that changes in one part of the system can cascade and impact other parts.\n\n\nTransfer Learning and Cascading Effects\nTransfer learning, a concept where a model trained for one task is repurposed for another, is highlighted as an example of this cascading effect. If you base your model on another model (e.g., using a pre-trained image recognition model for a new task), you inherit not only the benefits but also the potential issues of the original model. This can lead to cascading challenges if the original model changes or has underlying issues.\n\n\nSilent Consumers and the Impact of Changes\nAnother challenge arises when multiple models or systems are interdependent. For instance, if your model’s output is used by another system (a “silent consumer”), changes in your model can inadvertently impact that downstream system. Similarly, if your model relies on data from another source and that source changes, it can affect your model’s performance."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#correction-cascades",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#correction-cascades",
    "title": "Data Intensive Applications - Introduction",
    "section": "Correction Cascades",
    "text": "Correction Cascades\nThe lecture touches upon the concept of correction cascades. Using an illustrative example, imagine you have a model (Model A) that predicts the temperature for the next day. Another model (Model B) uses the output from Model A to predict how many bicycles will be needed at a rental station based on the forecasted temperature. If Model A’s predictions change or are inaccurate, it directly impacts Model B’s predictions. This interconnected reliance and the potential for errors to cascade from one model to another exemplify the challenges of correction cascades."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-dependencies-and-declared-consumers",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#data-dependencies-and-declared-consumers",
    "title": "Data Intensive Applications - Introduction",
    "section": "Data Dependencies and Declared Consumers",
    "text": "Data Dependencies and Declared Consumers\nThe lecture continues to explore the intricacies of data dependencies and the importance of declared consumers in machine learning systems.\n\nData Dependencies\nData dependencies play a pivotal role in machine learning and data-intensive applications. The lecture emphasizes the need for understanding and managing these dependencies. For instance, if you’re consuming data from another model or system, it’s crucial to be aware of any changes in that data source. Even subtle changes in the data distribution can have cascading effects on your model’s performance.\n\n\nDeclared vs. Undeclared Consumers\nThe concept of declared and undeclared consumers is introduced. Declared consumers are those that are explicitly known and have established contracts or interfaces with the data provider. In contrast, undeclared consumers might be using the data without the provider’s knowledge. While declaring consumers is always preferable because it establishes clear contracts and expectations, even declared consumers can face challenges if the data they consume changes in unexpected ways.\n\n\nReducing Model Complexity\nThe lecture touches upon the importance of reducing model complexity, especially when dealing with high-dimensional data. For instance, if you have features that are highly correlated, it might be beneficial to reduce the feature set’s size. This can lead to a simpler model that’s easier to interpret and manage, without significantly compromising predictive performance.\n\n\nThe Importance of Monitoring\nA recurring theme in the lecture is the need for robust monitoring. Given the complexities and potential pitfalls in data-intensive applications, having a comprehensive monitoring system in place is crucial. Such a system can help detect changes in data distributions, performance drifts, and other anomalies that might affect the model’s performance.\nThe lecture emphasizes that while machine learning models can offer powerful predictive capabilities, they’re just one part of a broader system. Ensuring that all components, from data ingestion to monitoring, work harmoniously is essential to avoid pitfalls and ensure robust performance."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#feedback-loops-and-complex-systems",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#feedback-loops-and-complex-systems",
    "title": "Data Intensive Applications - Introduction",
    "section": "Feedback Loops and Complex Systems",
    "text": "Feedback Loops and Complex Systems\nThe lecture delves into the intricacies of feedback loops in machine learning systems, especially when these systems become complex. Feedback loops can introduce biases and unexpected behaviors in models, making it essential to be aware of and manage them.\n\nRecommendation Systems and E-commerce Reviews\nAn illustrative example is presented involving recommendation systems in e-commerce platforms. Imagine a scenario where a recommendation system suggests products to users. These recommendations might influence user reviews on the platform. In turn, these reviews can bias the recommendation system, creating a feedback loop. The system’s recommendations are influenced by the reviews, and the reviews are influenced by the recommendations.\nThis example underscores the importance of understanding the context of data collection. The context can change the data distribution, potentially affecting the model’s performance. It’s crucial to be skeptical and continuously challenge the context to identify and manage such feedback loops.\n\n\nThe Importance of Context\nThe lecture emphasizes that the context of data collection matters significantly. Changes in context can alter data distributions, which can, in turn, affect model performance. Being vigilant and continuously challenging the context is essential to ensure the model remains robust and performs as expected."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#drawing-parallels-with-software-development",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#drawing-parallels-with-software-development",
    "title": "Data Intensive Applications - Introduction",
    "section": "Drawing Parallels with Software Development",
    "text": "Drawing Parallels with Software Development\nThe lecture draws parallels between machine learning system development and traditional software development. While software development has matured over the years, machine learning system development is still evolving. However, tools like Spark, Delta Lake, and MLflow, which you’ll be learning about in this course, are designed to address the challenges discussed.\nThese tools provide solutions to the problems faced in machine learning system development, making the process more streamlined and efficient. The lecture emphasizes that adopting industry-standard platforms can help combat many of the challenges associated with machine learning systems.\n\nAnti-patterns in Software and Machine Learning Systems\nThe lecture touches upon anti-patterns, which are common solutions to recurring problems that are counterproductive. These anti-patterns can manifest in both traditional software development and machine learning systems. The tools and platforms discussed aim to address these anti-patterns, ensuring that the development process remains efficient and effective.\n\n\nThe Evolution of Tools and Platforms\nReflecting on the past, the lecture highlights that a decade ago, many of the tools and platforms available today didn’t exist. Machine learning developers had to build many components from scratch or face the consequences of not having these tools. The evolution of these tools and platforms has made the development process more streamlined, allowing developers to focus on building robust and efficient machine learning systems."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#cultural-debt-and-team-dynamics",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#cultural-debt-and-team-dynamics",
    "title": "Data Intensive Applications - Introduction",
    "section": "Cultural Debt and Team Dynamics",
    "text": "Cultural Debt and Team Dynamics\nThe lecture introduces the concept of cultural debt, which arises from the dynamics and structures within organizations. This debt can manifest in various ways, especially when different teams or departments have distinct ways of doing things.\n\nSeparation of R&D and Engineering\nA common scenario highlighted is the separation of the R&D team, which focuses on machine learning development, from the engineering team responsible for deploying those models. This separation can create a chasm, making the handoff between R&D and engineering a potential liability. If not managed effectively, this can lead to miscommunication, misalignment, and inefficiencies in the development and deployment process.\n\n\nThe Value of Heterogeneous Teams\nDrawing from experience, the lecture emphasizes the benefits of creating heterogeneous teams where members work cross-functionally. Such teams combine various roles and expertise, ensuring a more holistic approach to problem-solving. This setup aligns with the diagram discussed in the first lecture, which highlighted the diverse roles involved in building data-intensive applications.\nIn heterogeneous teams, everyone is on the same page, understanding the challenges and intricacies of both the development and deployment processes. This collaborative approach can mitigate many of the challenges associated with cultural debt, ensuring smoother transitions and more efficient workflows.\n\n\nThe Importance of Collaboration\nThe lecture underscores the importance of fostering a collaborative environment. When everyone is involved in the process, from the initial stages of model development to its final deployment, it ensures a more cohesive and efficient approach. Collaboration helps in bridging gaps, understanding different perspectives, and ensuring that the final product aligns with the organization’s goals and objectives."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#moving-in-unison-and-data-management",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#moving-in-unison-and-data-management",
    "title": "Data Intensive Applications - Introduction",
    "section": "Moving in Unison and Data Management",
    "text": "Moving in Unison and Data Management\nThe lecture emphasizes the importance of moving in unison when building and maintaining machine learning applications, especially at scale. The course aims to introduce platforms and practices that ensure reliability and scalability in these applications.\n\nBronze, Silver, and Gold Data Repositories\nYou’re introduced to the concept of data repositories categorized as bronze, silver, and gold. When ingesting data, it’s initially stored in its raw form in a bronze repository. This raw data is essential as it serves as the foundation for subsequent transformations and analyses. From the bronze data, multiple silver data sources can be produced, which might undergo further transformations or aggregations. The gold repository represents the most refined and processed data, ready for model training or business analytics.\n\n\nConcurrent Data Access and ACID Properties\nThe lecture highlights the significance of concurrent data access, especially when multiple teams or systems are working on the same data source. The ACID (Atomicity, Consistency, Isolation, Durability) properties of databases ensure that data remains consistent and reliable, even when accessed concurrently by multiple entities.\n\n\nModel Staging and Deployment\nThe process of model staging and deployment is discussed. In a typical machine learning workflow, there might be a model currently in production, while another model is being developed or refined. The ability to seamlessly transition from one model to another without disrupting the application is crucial. This “hot handoff” ensures that the application remains functional even as models are updated or replaced.\nMoreover, the lecture touches upon the importance of having a fallback mechanism. If a newly deployed model doesn’t perform as expected, there should be a mechanism to revert to a previous, stable model seamlessly.\n\n\nEnsuring Functionality and Documentation\nThe ultimate goal is to ensure that the machine learning system functions precisely as specified. Proper documentation and a controlled environment are essential to achieve this. The lecture emphasizes that while building the model is a significant step, ensuring its consistent and reliable performance in a real-world environment is equally, if not more, important."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#cultural-differences-between-teams",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#cultural-differences-between-teams",
    "title": "Data Intensive Applications - Introduction",
    "section": "Cultural Differences Between Teams",
    "text": "Cultural Differences Between Teams\nThe lecture delves deeper into the cultural differences that can arise between teams, especially when they operate using different tools, languages, or methodologies.\n\nJava vs. Python: A Case Study\nA real-world example is presented, highlighting the challenges faced by a company named Venomics. The software development team at Venomics, responsible for cloud services and model deployment, predominantly used Java. In contrast, the team that built the machine learning models operated in Python, given its seamless integration with various machine learning frameworks.\nThis dichotomy led to challenges when deploying Python-based models to a Java-centric environment. For instance, if the Python team altered the feature vector (either its order or number) without notifying the Java team, it could lead to runtime failures in the Java code. This issue arises because there’s no compile-time binding between the Python model and the Java client using it.\nTo address this challenge, Venomics developed software that allowed Java clients to test feature vectors at runtime. This ensured that the deployed model’s interface remained consistent and didn’t break the contract between the Java clients and the Python models.\n\n\nThe Importance of Shared Libraries\nIf both teams had been using Python, they could have leveraged shared libraries to encapsulate the feature vector used in the model. This would have provided a common interface for both the model development and client-side, reducing the chances of inconsistencies.\n\n\nTest-Driven Development and Communication\nThe lecture underscores the importance of test-driven development and effective communication between teams. By adopting a test-driven approach and ensuring open communication channels, many of the challenges arising from cultural differences can be mitigated.\n\n\nIntegrating Machine Learning Scientists with Other Teams\nFor organizations aiming to derive value from machine learning applications, it’s essential not to isolate machine learning scientists or data scientists. Integrating them with other teams and fostering a collaborative environment can lead to more efficient workflows and better outcomes."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#team-dynamics-and-organizational-culture",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#team-dynamics-and-organizational-culture",
    "title": "Data Intensive Applications - Introduction",
    "section": "Team Dynamics and Organizational Culture",
    "text": "Team Dynamics and Organizational Culture\nThe lecture delves into the importance of team dynamics and the broader organizational culture when building and deploying machine learning systems.\n\nHeterogeneous vs. Homogeneous Teams\nYou’re introduced to the concept of heterogeneous and homogeneous teams. Heterogeneous teams consist of members with diverse roles and expertise, working collaboratively. In contrast, homogeneous teams might consist of members with similar roles or expertise, with defined handoffs between teams. The lecture emphasizes the benefits of heterogeneous teams, where cross-pollination of ideas and expertise can lead to more efficient problem-solving.\n\n\nInterviewing and Team Dynamics\nA practical tip shared is to inquire about team dynamics during job interviews. By understanding whether teams are heterogeneous or homogeneous and how they manage handoffs, you can gauge the organization’s approach to collaboration and problem-solving."
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#dia-process-references",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#dia-process-references",
    "title": "Data Intensive Applications - Introduction",
    "section": "DIA Process References",
    "text": "DIA Process References\n\n12-steps-to-applied-ai\nRules of ML\nPractical Advice for Large Datasets\nHidden Technical Debt in Machine Learning Systems\nData Validation For Machine Learning"
  },
  {
    "objectID": "posts/DoingDataAtScale/Data Intensive Applications/index.html#dia-platforms-reference",
    "href": "posts/DoingDataAtScale/Data Intensive Applications/index.html#dia-platforms-reference",
    "title": "Data Intensive Applications - Introduction",
    "section": "DIA Platforms Reference",
    "text": "DIA Platforms Reference\n\nResilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing\nDiscretized Streams: Fault-Tolerant Streaming Computation at Scale\nDelta Lake: High-Performance ACID Table Storage over Cloud Object Stores\nAccelerating the Machine Learning Lifecycle with MLflow"
  },
  {
    "objectID": "posts/DoingDataAtScale/Introduction/index.html",
    "href": "posts/DoingDataAtScale/Introduction/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Doing Data at Scale"
  },
  {
    "objectID": "posts/DoingDataAtScale/Introduction/index.html#modules",
    "href": "posts/DoingDataAtScale/Introduction/index.html#modules",
    "title": "Introduction",
    "section": "Modules",
    "text": "Modules\n\nData intensive applications (DIA\nSpark overview (distributed compute)\nSpark Optimization - spark simulator\nData Engineering\nMachine Learning Operations\nEnd 2 End Application Development\n\nData intensive applications combine a large complement of data and compute to augment or replace human capacity for the benefit of optimizing important processes."
  },
  {
    "objectID": "posts/DoingDataAtScale/Spark/index.html",
    "href": "posts/DoingDataAtScale/Spark/index.html",
    "title": "Spark- Overview",
    "section": "",
    "text": "Spark"
  },
  {
    "objectID": "posts/DoingDataAtScale/Spark/index.html#why-is-spark-important-to-data-driven-application-development",
    "href": "posts/DoingDataAtScale/Spark/index.html#why-is-spark-important-to-data-driven-application-development",
    "title": "Spark- Overview",
    "section": "Why is Spark Important to Data-Driven application development?",
    "text": "Why is Spark Important to Data-Driven application development?\n\nScalability: In the era of big data, the ability to process vast amounts of data efficiently is paramount. Spark provides a platform that can scale to handle petabytes of data, making it a go-to solution for large-scale data processing.\nPerformance: Spark’s in-memory computation capabilities can process data quickly. This is especially crucial for data-driven applications where real-time or near-real-time processing is required.\nFlexibility: Unlike some other big data processing frameworks, Spark supports batch processing, interactive queries, streaming, and machine learning, all under one roof. This versatility makes it an ideal choice for various data-driven applications."
  },
  {
    "objectID": "posts/DoingDataAtScale/Spark/index.html#innovations-spark-brought-vs.-the-previous-state-of-the-art",
    "href": "posts/DoingDataAtScale/Spark/index.html#innovations-spark-brought-vs.-the-previous-state-of-the-art",
    "title": "Spark- Overview",
    "section": "Innovations Spark Brought vs. The Previous State of the Art",
    "text": "Innovations Spark Brought vs. The Previous State of the Art\n\nIn-Memory Computation: One of Spark’s most significant innovations is its ability to store data in memory. This contrasts with the disk-based storage approach of many earlier systems, leading to much faster data retrieval and processing times.\nResilient Distributed Datasets (RDDs): RDDs are a fundamental data structure in Spark. They allow data to be distributed across a cluster and processed in parallel. RDDs are fault-tolerant, meaning they can recover from node failures, ensuring data integrity and system reliability.\nUnified Platform: Before Spark, developers often had to use a patchwork of different tools for various big data tasks. Spark combined these capabilities into a single platform, simplifying the development process and reducing the need for multiple tools."
  },
  {
    "objectID": "posts/DoingDataAtScale/Spark/index.html#main-features-of-the-spark-platform",
    "href": "posts/DoingDataAtScale/Spark/index.html#main-features-of-the-spark-platform",
    "title": "Spark- Overview",
    "section": "Main Features of the Spark Platform",
    "text": "Main Features of the Spark Platform\n\nSpark Core: At the heart of Spark is its core engine, which provides the fundamental functionalities of the platform, including task scheduling, memory management, and fault recovery.\nSpark SQL: This module allows users to execute SQL-like queries on their data, making it easier for those familiar with SQL to work with big data in Spark.\nSpark Streaming: For applications that require real-time data processing, Spark Streaming offers the ability to process live data streams, such as those from social media or IoT devices.\nMLlib: Machine learning is a crucial component of many data-driven applications. MLlib is Spark’s machine learning library, providing a range of algorithms and tools for data analysis and model training.\nGraphX: For applications that deal with graph data, GraphX offers a suite of graph computation tools and algorithms.\n\nIn conclusion, Apache Spark has revolutionized the world of big data processing. Its innovations and features have made it an indispensable tool for developers building data-driven applications. As data grows in volume and importance, platforms like Spark will remain at the forefront of the big data revolution."
  },
  {
    "objectID": "posts/DoingDataAtScale/Spark/index.html#so-how-does-this-spark-platfrom-evolution-play-out-in-code",
    "href": "posts/DoingDataAtScale/Spark/index.html#so-how-does-this-spark-platfrom-evolution-play-out-in-code",
    "title": "Spark- Overview",
    "section": "So how does this Spark Platfrom evolution play out in code…",
    "text": "So how does this Spark Platfrom evolution play out in code…\nConsider three examples of how to count filler words in a text file to illustrate the comparison - RDD based pyspark program - SQL/Dataframe pyspark program - Dataframe Pandas program (as an example that is likely fimiliar to data science students)\n\nUsing RDDs …\n\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\nfrom pyspark import SparkContext, SparkConf\nimport string\n\n# Initialize Spark\nconf = SparkConf().setAppName(\"FillerWordsCountRDD\")\nsc = SparkContext(conf=conf)\n\n# Define filler words\nfiller_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n\n# Read the text file\nrdd = sc.textFile(\"example.txt\")\n\n# Process and count filler words\ncounts = (rdd.flatMap(lambda line: line.split(\" \"))\n          .map(lambda word: word.lower().translate(str.maketrans('', '', string.punctuation)))\n          .filter(lambda word: word in filler_words)\n          .countByValue()\n          )\n\n# Print the results\nfor word, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"{word}: {count}\")\n\n# Stop Spark\nsc.stop()\n\n23/10/31 13:16:22 WARN Utils: Your hostname, lpalum-precision-5520 resolves to a loopback address: 127.0.1.1; using 192.168.1.172 instead (on interface wlp1s0)\n23/10/31 13:16:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n23/10/31 13:16:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\num: 3\nuh: 3\nso: 3\nlike: 2\n\n\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n                                                                                \n\n\n\n\nUsing SQL Dataframes …\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode, split, lower, regexp_replace, col, udf\nfrom pyspark.sql.types import StringType\nimport re\n\n# define a UDF for removing the punctuation\nremovePunctUDF = udf(lambda x:re.sub('[^A-Za-z\\s\\d]', \"\",x),StringType()) \n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"FillerWordsCountDF\").getOrCreate()\n\n# Define filler words\nfiller_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n\n# Read the text file into a DataFrame\ndf = spark.read.text(\"example.txt\").withColumn(\"word\", removePunctUDF(col(\"value\")))\n\n# Process and count filler words\nresult = (df.withColumn(\"word\", explode(split(col(\"word\"), \" \")))\n           .select(lower(col(\"word\")).alias(\"word\"))\n           .filter(col(\"word\").isin(filler_words))\n           .groupBy(\"word\")\n           .count()\n           .orderBy(\"count\", ascending=False))\n\n# Show the results\nresult.show()\n\n# Stop Spark\nspark.stop()\n\n                                                                                \n\n\n+----+-----+\n|word|count|\n+----+-----+\n|  uh|    3|\n|  um|    3|\n|  so|    3|\n|like|    2|\n+----+-----+\n\n\n\n\n\nUsing the Pandas Dataframe Library …\n\nimport pandas as pd\nimport string\nimport re\n\nf = open(\"example.txt\", \"r\")\ndf=pd.DataFrame(re.sub('[^A-Za-z\\s\\d]', \"\",f.read()).lower().split(), columns=['word'])\n\n# Define filler words\nfiller_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n\n# Apply the function to the DataFrame\ndf[\"filler_count\"] = df[\"word\"].apply(lambda w: 1 if w in filler_words else 0)\n\ndf.groupby('word').filler_count.sum()[df.groupby('word').filler_count.sum()&gt;0].sort_values(ascending=False)\n\nword\nso      3\nuh      3\num      3\nlike    2\nName: filler_count, dtype: int64"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "What Can Data Do?",
    "section": "",
    "text": "Welcome, data enthusiasts to “What Can Data Do?” – a digital realm where we embark on a never ending story that attempts to decode the mysteries and marvels of the data-driven world. I’m Lloyd Palum, your guide and the voice behind this blog.\n\n\n\n\n\nIn this age of information, we find ourselves surrounded by a constant stream of data. It flows through our devices, our businesses, and our daily lives. Data is more than just numbers and figures; it’s the key to unlocking the future. It’s the backbone of innovations, the heart of progress, and the foundation of insights that drive both individual curiosity and business success.\nAs the CTO at Tensteet (a transportation analytics company), an Adjunct Instructor at the Georgen Institute of Data Science, and a passionate explorer of Machine Learning, Software Development, IoT, and Cloud Computing, I’ve been on a road through the vast data landscape. My knowledge, experiences, and occasional eureka moments are what I want to share with you through this blog.\nIn the era of ‘big data,’ where information is power, ‘What Can Data Do?’ becomes the central question. It’s a question that keeps data analysts and tech enthusiasts awake at night, and it’s a question that drives our quest for understanding. From leveraging data to make your business more efficient to uncovering the hidden patterns in the digital world, we’ll explore it all.\nBut, ‘What Can Data Do?’ is not just about the ‘how.’ It’s also about the ‘why.’ Why should you care about data? Why should you invest your time and energy in understanding its intricacies? Because, in data, we trust. Data is more than just numbers; it’s the foundation of knowledge, the essence of decision-making, and the bridge to innovation.\nSo, join me. Let’s unravel the potential of data, discuss the latest trends, tackle complex challenges, and dive deep into the world of data science. Whether you’re a seasoned data professional or just starting your data-driven adventure, this blog is your space to explore, learn, and engage.\n‘What Can Data Do?’ is not just a question. It’s an invitation to explore a world where data is king, and together, we’ll uncover the magic it holds.\nStay tuned for regular updates, insights, and discussions. In data, we trust.\nWelcome!\nProf. P."
  }
]