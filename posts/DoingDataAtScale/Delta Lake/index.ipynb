{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1afe1c41",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Delta Lake - Overview\"\n",
    "date: \"2023-10-03\"\n",
    "categories: [DeltaLake, Courses, DSAS]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a4f9c",
   "metadata": {},
   "source": [
    "![Delta Lake](deltalake.png)\n",
    "---\n",
    "- [Delta Lake: High-Performance ACID Table Storage over\n",
    "Cloud Object Stores](https://www.vldb.org/pvldb/vol13/p3411-armbrust.pdf)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d969b-902b-4a15-8a55-3e3cdb120e37",
   "metadata": {},
   "source": [
    "## Key Features of Delta Lake:\n",
    "1. **ACID Transactions**: Delta Lake provides full ACID transactional capabilities, ensuring data integrity and consistency.\n",
    "2. **Scalable Metadata Handling**: It uses a unique approach to handle metadata, allowing it to scale to billions of partitions and trillions of files.\n",
    "3. **Unified Batch and Streaming**: Delta Lake supports both batch and streaming workloads with a unified experience.\n",
    "4. **Schema Evolution**: It allows for schema-on-read and schema-on-write, enabling users to evolve their datasets over time.\n",
    "5. **Audit History**: Delta Lake maintains a detailed audit trail of all changes, allowing for versioning and time-travel capabilities.\n",
    "6. **Support for Deletes, Updates, and Merges**: Delta Lake supports operations like DELETE, UPDATE, and MERGE INTO, which are typically not available in most data lakes.\n",
    "\n",
    "## Problems Delta Lake Solves:\n",
    "1. **Data Reliability**: Traditional data lakes often suffer from issues like data corruption, missing files, or duplicate data. Delta Lake's ACID transactions eliminate these problems.\n",
    "2. **Performance**: By leveraging Spark's capabilities and optimizing storage layer operations, Delta Lake provides faster query performance.\n",
    "3. **Complex ETL Workflows**: Delta Lake simplifies ETL workflows by supporting operations like upserts and deletes.\n",
    "4. **Metadata Scalability**: Handling metadata for large datasets can be challenging. Delta Lake's approach to metadata ensures scalability and performance.\n",
    "\n",
    "## Significance of Delta Lake in the Data Stack:\n",
    "1. **Enhanced Data Reliability**: By providing ACID transactions, Delta Lake ensures that the data is reliable and consistent, which is crucial for analytical workloads.\n",
    "2. **Flexibility**: With support for schema evolution and a wide range of operations, Delta Lake offers flexibility in managing and processing data.\n",
    "3. **Integration with Existing Tools**: Delta Lake seamlessly integrates with existing tools and frameworks, ensuring that organizations can adopt it without significant changes to their existing infrastructure.\n",
    "\n",
    "## Integration with Spark:\n",
    "1. **Native Integration**: Delta Lake is built on top of Spark, ensuring native integration and optimization.\n",
    "2. **Optimized Query Execution**: Delta Lake leverages Spark's Catalyst optimizer for efficient query execution.\n",
    "3. **Use of Spark APIs**: Users can utilize familiar Spark APIs to read and write data in Delta Lake, ensuring a smooth user experience.\n",
    "\n",
    "Delta Lake is a significant addition to the data stack as it addresses many of the challenges faced by traditional data lakes. By providing ACID transactions, scalable metadata handling, and a range of operations, Delta Lake ensures that data is reliable, consistent, and easily accessible. Its deep integration with Spark further enhances its capabilities, making it a powerful tool for data processing and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39193a8f-671f-4ba1-8ed4-839f118e4dc5",
   "metadata": {},
   "source": [
    "ACID is an acronym that stands for Atomicity, Consistency, Isolation, and Durability. These are a set of properties that ensure reliable processing of database transactions. Let's delve into each of these properties:\n",
    "\n",
    "### 1. Atomicity:\n",
    "**Definition**: Atomicity ensures that a transaction is treated as a single, indivisible unit, which means either all of its operations are executed or none of them are. If a transaction is interrupted (for example, due to a system crash or power failure), any changes that it made are rolled back to ensure the database remains in a consistent state.\n",
    "\n",
    "**Example**: Consider a banking application where you are transferring money from one account to another. This transaction involves two operations: \n",
    "- Deducting the amount from the source account.\n",
    "- Adding the amount to the destination account.\n",
    "\n",
    "If the system fails after deducting the amount but before adding it to the destination account, atomicity ensures that the deducted amount is rolled back to the source account, ensuring no money is lost.\n",
    "\n",
    "### 2. Consistency:\n",
    "**Definition**: Consistency ensures that a transaction brings the database from one valid state to another. After a transaction has been committed, the changes are permanent, and the database will be left in a consistent state. Any transaction that would violate the database's consistency rules is rolled back.\n",
    "\n",
    "**Example**: In the same banking application, suppose there's a rule that an account balance should never go below $100. If a transaction tries to withdraw an amount that would violate this rule, the transaction would be rolled back, ensuring the database remains consistent.\n",
    "\n",
    "### 3. Isolation:\n",
    "**Definition**: Isolation ensures that concurrent transactions are executed in such a way that the results are the same as if the transactions were executed serially, one after the other. This means that the intermediate state of a transaction is invisible to other transactions.\n",
    "\n",
    "**Example**: Imagine two transactions: \n",
    "- Transaction A reads a value and increases it by $10.\n",
    "- Transaction B reads the same value simultaneously and increases it by $20.\n",
    "\n",
    "If these transactions are not isolated, they might both read the same initial value, say $100, and update it to $110 and $120, respectively. With proper isolation, the final value would be $130, as one transaction would wait for the other to complete before reading the updated value.\n",
    "\n",
    "### 4. Durability:\n",
    "**Definition**: Durability ensures that once a transaction has been committed, its effects are permanent, even in the case of system failures. This is typically achieved by storing transaction logs or using backup mechanisms.\n",
    "\n",
    "**Example**: After completing a purchase in an online store, the transaction details are written to the database. Even if the system crashes immediately after, durability ensures that the transaction details are not lost and can be recovered when the system restarts.\n",
    "\n",
    "ACID properties are fundamental to ensuring the reliability and robustness of database systems. They ensure that the data remains consistent and intact even in the face of system failures or concurrent access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7755941-b6eb-48cd-880d-4a506a168320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50797edd-7ee8-4aa8-a635-dea79d3f1e45",
   "metadata": {},
   "source": [
    "## Create a Delta Table\n",
    "- Underlying format uses columnar Parquet Files\n",
    "- JSON delta log is used by the delta lake transactional core software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee57ef1-ad20-4e6a-a5b9-4d3125ec84c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.range(0, 5)\n",
    "data.write.format(\"delta\").save(\"./delta-table\", OVERWRITE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba4234a-cdf2-4d72-9cf9-e647f9237cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_delta_log\n",
      "part-00000-aa7ed3d3-662a-46cf-b94f-f90235a78d46-c000.snappy.parquet\n",
      "part-00001-c57f57a0-c72a-43ad-8529-a37859acd44f-c000.snappy.parquet\n",
      "part-00002-50f9305f-41cc-43e6-b532-7f4c50233b3f-c000.snappy.parquet\n",
      "part-00003-19ed009f-69d1-49ce-86f1-312cb074b6a3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls delta-table/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e94011-6f48-41ad-b9e0-3618b986387b",
   "metadata": {},
   "source": [
    "## Read the delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6267ed-7d38-4556-b91c-fecac92a2ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  4|\n",
      "|  2|\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"./delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97303d00-c5df-4a30-acc2-b89b9f8cb76b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Update table - overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faeb45a9-17cd-46c6-917a-ac0855813b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.range(5, 10)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"./delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05971f5b-c230-47a9-a53f-454f7bf86cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  8|\n",
      "|  9|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"./delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c97f20-1d3a-415a-b356-8ba5574b3ed4",
   "metadata": {},
   "source": [
    "## Conditional update without overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40fabf5a-00e5-4385-b07a-be685f1d75f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6103ac-cd31-44df-a90e-4d56d8ac7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"./delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ee264f-c6d6-4dcc-b470-2482ff985bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update every even value by adding 100 to it\n",
    "deltaTable.update(condition=expr(\"id % 2 == 0\"), set={\"id\": expr(\"id + 100\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c15350-01af-4b8f-a5a8-b971c19445f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|108|\n",
      "|  9|\n",
      "|  5|\n",
      "|  7|\n",
      "|106|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1058e96a-0d04-4c6f-a3aa-b438e6032272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Delete every even value\n",
    "deltaTable.delete(condition=expr(\"id % 2 == 0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f7e31c-7f99-4cb4-a4aa-28ce4e859406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  9|\n",
      "|  5|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2133e3cd-ab14-407e-a293-110dc7ae5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Upsert (merge) new data\n",
    "newData = spark.range(0, 20)\n",
    "\n",
    "deltaTable.alias(\"oldData\").merge(\n",
    "    newData.alias(\"newData\"), \"oldData.id = newData.id\"\n",
    ").whenMatchedUpdate(set={\"id\": col(\"newData.id\")}).whenNotMatchedInsert(\n",
    "    values={\"id\": col(\"newData.id\")}\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5827d0dc-4208-4a3b-9511-c523db10c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486c487-ce47-4092-ac38-e66bcdf61f30",
   "metadata": {},
   "source": [
    "## Read older versions of data using time travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a30da9-47f3-4be6-aa50-73677371d68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  4|\n",
      "|  2|\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"./delta-table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc8fae-3a2a-450b-8672-a1ec7fb8a3da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Looking at the underlying transaction log maintained by Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5771f64b-0c83-42b1-90a4-f744d2eb0278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"remove\":{\"path\":\"part-00000-ad39a0c3-57cd-4b19-bd34-c1322d1f026e-c000.snappy.parquet\",\"deletionTimestamp\":1698778757438,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":486}}\n",
      "{\"remove\":{\"path\":\"part-00001-a61ba60f-f4eb-40c1-ab50-88fa3600e94a-c000.snappy.parquet\",\"deletionTimestamp\":1698778757438,\"dataChange\":true,\"extendedFileMetadata\":true,\"partitionValues\":{},\"size\":478}}\n",
      "{\"add\":{\"path\":\"part-00000-dbf8c991-8a1a-42b4-9b3e-93dcb13801fa-c000.snappy.parquet\",\"partitionValues\":{},\"size\":478,\"modificationTime\":1698778757430,\"dataChange\":true,\"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":9},\\\"maxValues\\\":{\\\"id\\\":9},\\\"nullCount\\\":{\\\"id\\\":0}}\"}}\n",
      "{\"commitInfo\":{\"timestamp\":1698778757453,\"operation\":\"DELETE\",\"operationParameters\":{\"predicate\":\"[\\\"((id % 2L) = 0L)\\\"]\"},\"readVersion\":2,\"isolationLevel\":\"Serializable\",\"isBlindAppend\":false,\"operationMetrics\":{\"numRemovedFiles\":\"2\",\"numCopiedRows\":\"1\",\"numAddedChangeFiles\":\"0\",\"executionTimeMs\":\"899\",\"numDeletedRows\":\"2\",\"scanTimeMs\":\"722\",\"numAddedFiles\":\"1\",\"rewriteTimeMs\":\"177\"},\"engineInfo\":\"Apache-Spark/3.3.1 Delta-Lake/2.1.0\",\"txnId\":\"951deaaf-e06b-464a-bbf1-a2e6bc62efb7\"}}\n"
     ]
    }
   ],
   "source": [
    "!cat ./delta-table/_delta_log/00000000000000000003.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
