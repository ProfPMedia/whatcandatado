{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b9d4b321",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark- Overview\"\n",
    "date: \"2023-10-02\"\n",
    "categories: [Spark, Courses, DSAS]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e17b3",
   "metadata": {},
   "source": [
    "![Spark](spark.png)\n",
    "---\n",
    "- [Link to the Paper: Resilient Distributed Datasets: A Fault-Tolerant Abstraction for\n",
    "In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n",
    "- [Link to the Paper: Spark SQL: Relational Data Processing in Spark](https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf)\n",
    "- [Apache Spark Docmentation - A Unified engine for large-scale data analytics](https://spark.apache.org/docs/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe72e3-e482-4fe8-8c56-d21160e9f9e6",
   "metadata": {},
   "source": [
    "# Understanding Apache Spark: A Deep Dive into Data-Driven Applications\n",
    "\n",
    "Apache Spark, a fast and general-purpose cluster-computing system, has become a cornerstone in big data processing. This article delves into its significance, innovations, and core features, drawing insights from the seminal paper presented at NSDI'12.\n",
    "\n",
    "## Why is Spark Important to Data-Driven application development?\n",
    "\n",
    "- **Scalability**: In the era of big data, the ability to process vast amounts of data efficiently is paramount. Spark provides a platform that can scale to handle petabytes of data, making it a go-to solution for large-scale data processing.\n",
    "\n",
    "- **Performance**: Spark's in-memory computation capabilities can process data quickly. This is especially crucial for data-driven applications where real-time or near-real-time processing is required.\n",
    "\n",
    "- **Flexibility**: Unlike some other big data processing frameworks, Spark supports batch processing, interactive queries, streaming, and machine learning, all under one roof. This versatility makes it an ideal choice for various data-driven applications.\n",
    "\n",
    "## Innovations Spark Brought vs. The Previous State of the Art\n",
    "\n",
    "- **In-Memory Computation**: One of Spark's most significant innovations is its ability to store data in memory. This contrasts with the disk-based storage approach of many earlier systems, leading to much faster data retrieval and processing times.\n",
    "\n",
    "- **Resilient Distributed Datasets (RDDs)**: RDDs are a fundamental data structure in Spark. They allow data to be distributed across a cluster and processed in parallel. RDDs are fault-tolerant, meaning they can recover from node failures, ensuring data integrity and system reliability.\n",
    "\n",
    "- **Unified Platform**: Before Spark, developers often had to use a patchwork of different tools for various big data tasks. Spark combined these capabilities into a single platform, simplifying the development process and reducing the need for multiple tools.\n",
    "\n",
    "## Main Features of the Spark Platform\n",
    "\n",
    "- **Spark Core**: At the heart of Spark is its core engine, which provides the fundamental functionalities of the platform, including task scheduling, memory management, and fault recovery.\n",
    "\n",
    "- **Spark SQL**: This module allows users to execute SQL-like queries on their data, making it easier for those familiar with SQL to work with big data in Spark.\n",
    "\n",
    "- **Spark Streaming**: For applications that require real-time data processing, Spark Streaming offers the ability to process live data streams, such as those from social media or IoT devices.\n",
    "\n",
    "- **MLlib**: Machine learning is a crucial component of many data-driven applications. MLlib is Spark's machine learning library, providing a range of algorithms and tools for data analysis and model training.\n",
    "\n",
    "- **GraphX**: For applications that deal with graph data, GraphX offers a suite of graph computation tools and algorithms.\n",
    "\n",
    "In conclusion, Apache Spark has revolutionized the world of big data processing. Its innovations and features have made it an indispensable tool for developers building data-driven applications. As data grows in volume and importance, platforms like Spark will remain at the forefront of the big data revolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db168e-8155-40f6-bc76-591464edb482",
   "metadata": {},
   "source": [
    "# Article: Spark's Evolution: Embracing SQL and DataFrames**\n",
    "\n",
    "Apache Spark, a fast and general-purpose cluster-computing system, has been at the forefront of big data processing. Over the years, it has seen numerous updates, each aiming to make it more efficient, scalable, and user-friendly. One of the most significant updates in recent times has been the introduction of SQL and DataFrames. This article delves into the details of this update, as described in the paper by Armbrust et al. from MIT CSAIL.\n",
    "\n",
    "### 1. **Introduction to the Update**\n",
    "\n",
    "Spark's initial API was based on Resilient Distributed Datasets (RDDs). While powerful, RDDs required users to manually optimize their queries, which could be cumbersome. The new update introduces two high-level APIs: SQL and DataFrames, which aim to provide users with the ability to express computations concisely and have the system optimize them.\n",
    "\n",
    "### 2. **What's New?**\n",
    "\n",
    "- **Unified Data Processing**: Spark SQL provides a unified means of accessing structured data. Whether the data resides in Parquet, JSON, Hive, or other sources, users can query it seamlessly using SQL.\n",
    "\n",
    "- **DataFrame API**: Inspired by data frames in R and Python (with Pandas), Spark's DataFrame API provides operations to filter, aggregate, and compute statistics on large datasets. It's a distributed collection of data organized into named columns, making it easier to handle and process.\n",
    "\n",
    "- **Optimized Execution**: The Catalyst optimizer is at the heart of Spark SQL. It optimizes SQL queries as well as DataFrame operations, ensuring efficient execution. Catalyst uses advanced programming language features (like Scala's pattern matching) to build an extensible query optimizer.\n",
    "\n",
    "![Catalyst Optimizer](https://www.databricks.com/wp-content/uploads/2018/05/Catalyst-Optimizer-diagram.png)\n",
    "\n",
    "- **Interoperability**: Users can seamlessly mix SQL queries with Spark programs. This means they can use RDDs, DataFrames, and SQL interchangeably, depending on the use case.\n",
    "\n",
    "### 3. **Improvements Over the Previous Generation**\n",
    "\n",
    "- **Performance**: With the Catalyst optimizer, Spark SQL can often outperform hand-optimized Spark programs. The optimizer ensures that the system understands the computation and can make intelligent decisions about its execution.\n",
    "\n",
    "- **Usability**: For those familiar with SQL, Spark SQL offers a more intuitive way to interact with data. Even for those who aren't, the DataFrame API provides a simpler, more expressive means of data manipulation.\n",
    "\n",
    "- **Flexibility**: The new APIs do not replace RDDs but rather complement them. Users who need the fine-grained control offered by RDDs can still use them, while also benefiting from the high-level abstractions of SQL and DataFrames.\n",
    "\n",
    "- **Extensibility**: Catalyst's rule-based optimizer is designed to be extensible, allowing developers to add new optimization techniques without altering the core. This ensures that Spark SQL can continue to evolve and adapt to new challenges.\n",
    "\n",
    "### 4. **Conclusion**\n",
    "\n",
    "The introduction of SQL and DataFrames in Spark marks a significant step forward in its evolution. By providing high-level abstractions, it makes big data processing more accessible to a broader audience. At the same time, with the Catalyst optimizer, it ensures that these high-level abstractions do not come at the cost of performance. As big data continues to grow in importance, tools like Spark that evolve to meet the challenges head-on will be invaluable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08db8d-e756-436e-848a-3e0daa36141c",
   "metadata": {},
   "source": [
    "## So how does this Spark Platfrom evolution play out in code...\n",
    "Consider three examples of how to count filler words in a text file to illustrate the comparison\n",
    "- RDD based pyspark program\n",
    "- SQL/Dataframe pyspark program\n",
    "- Dataframe Pandas program (as an example that is likely fimiliar to data science students)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6479a95-fc7a-4ff4-9506-2ddd54ffe611",
   "metadata": {},
   "source": [
    "### Using RDDs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "def3a365-ee9c-49a3-85d5-9349d801621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1efd6f-a680-416a-8fe6-3f7f209983b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/31 13:16:22 WARN Utils: Your hostname, lpalum-precision-5520 resolves to a loopback address: 127.0.1.1; using 192.168.1.172 instead (on interface wlp1s0)\n",
      "23/10/31 13:16:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/31 13:16:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "um: 3\n",
      "uh: 3\n",
      "so: 3\n",
      "like: 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import string\n",
    "\n",
    "# Initialize Spark\n",
    "conf = SparkConf().setAppName(\"FillerWordsCountRDD\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Define filler words\n",
    "filler_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n",
    "\n",
    "# Read the text file\n",
    "rdd = sc.textFile(\"example.txt\")\n",
    "\n",
    "# Process and count filler words\n",
    "counts = (rdd.flatMap(lambda line: line.split(\" \"))\n",
    "          .map(lambda word: word.lower().translate(str.maketrans('', '', string.punctuation)))\n",
    "          .filter(lambda word: word in filler_words)\n",
    "          .countByValue()\n",
    "          )\n",
    "\n",
    "# Print the results\n",
    "for word, count in sorted(counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop Spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f56ca1-416a-4811-8d80-0214f59c878c",
   "metadata": {},
   "source": [
    "### Using SQL Dataframes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308e4add-c498-40a1-9a62-03179d5d0a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|  uh|    3|\n",
      "|  um|    3|\n",
      "|  so|    3|\n",
      "|like|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# define a UDF for removing the punctuation\n",
    "removePunctUDF = udf(lambda x:re.sub('[^A-Za-z\\s\\d]', \"\",x),StringType()) \n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"FillerWordsCountDF\").getOrCreate()\n",
    "\n",
    "# Define filler words\n",
    "filler_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "df = spark.read.text(\"example.txt\").withColumn(\"word\", removePunctUDF(col(\"value\")))\n",
    "\n",
    "# Process and count filler words\n",
    "result = (df.withColumn(\"word\", explode(split(col(\"word\"), \" \")))\n",
    "           .select(lower(col(\"word\")).alias(\"word\"))\n",
    "           .filter(col(\"word\").isin(filler_words))\n",
    "           .groupBy(\"word\")\n",
    "           .count()\n",
    "           .orderBy(\"count\", ascending=False))\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde23172-8c5c-4bbd-9866-4039b60161a1",
   "metadata": {},
   "source": [
    "### Using the Pandas Dataframe Library ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab1b1b5-f888-4726-8d42-5846b40f6303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "so      3\n",
       "uh      3\n",
       "um      3\n",
       "like    2\n",
       "Name: filler_count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "f = open(\"example.txt\", \"r\")\n",
    "df=pd.DataFrame(re.sub('[^A-Za-z\\s\\d]', \"\",f.read()).lower().split(), columns=['word'])\n",
    "\n",
    "# Define filler words\n",
    "filler_words = [\"um\", \"uh\", \"like\", \"youknow\", \"so\", \"actually\", \"basically\", \"seriously\"]\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df[\"filler_count\"] = df[\"word\"].apply(lambda w: 1 if w in filler_words else 0)\n",
    "\n",
    "df.groupby('word').filler_count.sum()[df.groupby('word').filler_count.sum()>0].sort_values(ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-330-delta-210] *",
   "language": "python",
   "name": "conda-env-pyspark-330-delta-210-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
